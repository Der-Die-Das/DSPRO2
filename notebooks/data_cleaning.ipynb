{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file splits\n",
    "splits = {\n",
    "    'train': 'data/train-00000-of-00001.parquet',\n",
    "    'validation': 'data/validation-00000-of-00001.parquet',\n",
    "    'test': 'data/test-00000-of-00001.parquet'\n",
    "}\n",
    "\n",
    "dfs = {}\n",
    "# Load the train split from the Hugging Face dataset\n",
    "try:\n",
    "    dfs[\"train\"] = pd.read_parquet(\"hf://datasets/tau/commonsense_qa/\" + splits[\"train\"])\n",
    "    dfs[\"validation\"] = pd.read_parquet(\"hf://datasets/tau/commonsense_qa/\" + splits[\"validation\"])\n",
    "    dfs[\"test\"] = pd.read_parquet(\"hf://datasets/tau/commonsense_qa/\" + splits[\"test\"])\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to load dataset: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIRED_FIELDS = ['id', 'question', 'question_concept', 'choices', 'answerKey']\n",
    "\n",
    "def validate_entry(entry):\n",
    "    \"\"\"Ensure the entry has all required fields.\"\"\"\n",
    "    missing = [field for field in REQUIRED_FIELDS if field not in entry or pd.isna(entry[field])]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing fields {missing} in entry with id: {entry.get('id', 'unknown')}\")\n",
    "    return True\n",
    "\n",
    "def clean_entry(entry):\n",
    "    \"\"\"Perform basic cleaning on the entry and update choices.\"\"\"\n",
    "    try:\n",
    "        # Trim whitespace on basic string fields.\n",
    "        for key in ['id', 'question', 'question_concept', 'answerKey']:\n",
    "            if key in entry and isinstance(entry[key], str):\n",
    "                entry[key] = entry[key].strip()\n",
    "                \n",
    "        # Process the choices field if it is a dict.\n",
    "        if 'choices' in entry and isinstance(entry['choices'], dict):\n",
    "            # Define expected labels.\n",
    "            expected_labels = ['A', 'B', 'C', 'D', 'E']\n",
    "            \n",
    "            # Check for 'label' key and verify its content.\n",
    "            if 'label' in entry['choices']:\n",
    "                labels = entry['choices']['label']\n",
    "                # Convert numpy array to list if necessary.\n",
    "                if hasattr(labels, 'tolist'):\n",
    "                    labels = labels.tolist()\n",
    "                if labels != expected_labels:\n",
    "                    raise ValueError(f\"Unexpected labels: {labels}\")\n",
    "            \n",
    "            # Process the 'text' key, ensuring it's a list of strings.\n",
    "            if 'text' in entry['choices']:\n",
    "                # Clean the text entries and replace the choices dict with just the text list.\n",
    "                entry['choices'] = [choice.strip() for choice in entry['choices']['text']]\n",
    "            else:\n",
    "                raise ValueError(\"Missing 'text' in choices\")\n",
    "        else:\n",
    "            raise ValueError(\"Field 'choices' is missing or not a dict\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error cleaning entry {entry.get('id', 'unknown')}: {e}\")\n",
    "    return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All entries validated and cleaned successfully.\n",
      "Cleaned data saved to data/cleaned/train.csv\n",
      "All entries validated and cleaned successfully.\n",
      "Cleaned data saved to data/cleaned/validation.csv\n",
      "All entries validated and cleaned successfully.\n",
      "Cleaned data saved to data/cleaned/test.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Process each row in the DataFrame\n",
    "for key, df in dfs.items():\n",
    "    cleaned_entries = []\n",
    "    errors = []\n",
    "    for index, row in df.iterrows():\n",
    "        entry = row.to_dict()\n",
    "        try:\n",
    "            validate_entry(entry)\n",
    "            cleaned_entry = clean_entry(entry)\n",
    "            cleaned_entries.append(cleaned_entry)\n",
    "        except Exception as e:\n",
    "            errors.append(f\"Row {index}: {e}\")\n",
    "\n",
    "    if errors:\n",
    "        print(\"Some entries had issues:\")\n",
    "        for error in errors:\n",
    "            print(error)\n",
    "    else:\n",
    "        print(\"All entries validated and cleaned successfully.\")\n",
    "\n",
    "        # Convert cleaned entries to a DataFrame and save as CSV\n",
    "        cleaned_df = pd.DataFrame(cleaned_entries)\n",
    "        output_file = f'data/cleaned/{key}.csv'\n",
    "        cleaned_df.to_csv(output_file, index=False)\n",
    "        print(f\"Cleaned data saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
