{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 datasets evaluate sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdanielbetschart\u001b[0m (\u001b[33mdanielbetschart-hochschule-luzern\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 9741\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 1221\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 1140\n",
      "    })\n",
      "})\n",
      "\n",
      "Example Train instance:\n",
      "{'id': '075e483d21c29a511267ef62bedc0461', 'question': 'The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?', 'question_concept': 'punishing', 'choices': {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['ignore', 'enforce', 'authoritarian', 'yell at', 'avoid']}, 'answerKey': 'A'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"tau/commonsense_qa\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "print(\"Dataset loaded:\")\n",
    "print(dataset)\n",
    "print(\"\\nExample Train instance:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration & Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example Formatted Prompt (for training):\n",
      "### Question:\n",
      "The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
      "\n",
      "### Choices:\n",
      "A) ignore\n",
      "B) enforce\n",
      "C) authoritarian\n",
      "D) yell at\n",
      "E) avoid\n",
      "\n",
      "### Answer:\n",
      "A\n",
      "Formatting and tokenizer setup complete. Tokenization will occur within sweep runs.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer # Need tokenizer info early\n",
    "\n",
    "# --- Define formatting function ---\n",
    "def format_prompt(example):\n",
    "    question = example['question']\n",
    "    choices_text = example['choices']['text']\n",
    "    choices_labels = example['choices']['label']\n",
    "    answer_key = example['answerKey']\n",
    "\n",
    "    prompt = f\"### Question:\\n{question}\\n\\n### Choices:\\n\"\n",
    "    for label, text in zip(choices_labels, choices_text):\n",
    "        prompt += f\"{label}) {text}\\n\"\n",
    "    prompt += f\"\\n### Answer:\\n{answer_key}\"\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# Apply formatting\n",
    "formatted_dataset = dataset.map(format_prompt, remove_columns=list(dataset['train'].features))\n",
    "print(\"\\nExample Formatted Prompt (for training):\")\n",
    "print(formatted_dataset['train'][0]['text'])\n",
    "\n",
    "# --- Define Tokenization (but don't run tokenization globally yet) ---\n",
    "# We need the tokenizer object available, but tokenization will happen inside the sweep function\n",
    "base_model_name_for_tokenizer = \"tiiuae/falcon-7b-instruct\" # Needs to match model used in sweep\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name_for_tokenizer, token=None)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "max_sequence_length = 256 # Define max length here\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Use the globally defined tokenizer and max_sequence_length\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=max_sequence_length,\n",
    "    )\n",
    "\n",
    "# Note: We map the tokenize_function inside the train_sweep function later\n",
    "print(\"Formatting and tokenizer setup complete. Tokenization will occur within sweep runs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: kk2hcae9\n",
      "Sweep URL: https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/kk2hcae9\n",
      "Sweep ID: kk2hcae9\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes', # Or 'random', 'grid'\n",
    "    'metric': {\n",
    "      'name': 'eval/loss', # Optimize for lowest validation loss\n",
    "      'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 1e-5,\n",
    "            'max': 5e-4\n",
    "        },\n",
    "        'lora_r': {\n",
    "            'values': [16, 32, 64]\n",
    "        },\n",
    "        'lora_alpha': {\n",
    "             # Often set relative to r, e.g., 2*r, but let's try independent values\n",
    "            'values': [16, 32, 64]\n",
    "        },\n",
    "        'lora_dropout': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.05,\n",
    "            'max': 0.2\n",
    "        },\n",
    "        'gradient_accumulation_steps': {\n",
    "            'values': [4, 8]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [0.0, 0.001, 0.01]\n",
    "        }\n",
    "        # Add other parameters like per_device_train_batch_size if desired\n",
    "        # 'per_device_train_batch_size': {'values': [1, 2]}\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Initialize the Sweep ---\n",
    "# Make sure you are logged into W&B (`wandb login`)\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"falcon-commonsenseqa-sweep\") # Choose a project name\n",
    "print(f\"Sweep ID: {sweep_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. train_sweep function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    # AutoTokenizer loaded globally\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "# ===> IMPORT THIS <===\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import os\n",
    "import wandb # Make sure wandb is imported here too\n",
    "\n",
    "# Ensure global variables (tokenizer, formatted_dataset, tokenize_function, max_sequence_length)\n",
    "# are accessible within this function's scope. If running as a script, define them above or pass as args.\n",
    "\n",
    "def train_sweep():\n",
    "    run = None # Define run here to ensure it's accessible in finally block\n",
    "    try:\n",
    "        # Initialize W&B run for this specific sweep trial\n",
    "        run = wandb.init()\n",
    "        if run is None:\n",
    "             # Fallback for testing outside agent - provide dummy config\n",
    "             print(\"WARNING: wandb.init() returned None. Running with default test config.\")\n",
    "             class DummyConfig:\n",
    "                 learning_rate = 2e-4; lora_r = 64; lora_alpha = 16\n",
    "                 lora_dropout = 0.1; gradient_accumulation_steps = 8; weight_decay = 0.001\n",
    "             config = DummyConfig()\n",
    "             run = wandb.init(project=\"falcon-sweep-test\", config=vars(config))\n",
    "        else:\n",
    "             config = wandb.config\n",
    "\n",
    "        # --- Model Configuration ---\n",
    "        model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "        # --- QLoRA Configuration ---\n",
    "        use_4bit = True\n",
    "        bnb_4bit_compute_dtype = \"float16\"\n",
    "        bnb_4bit_quant_type = \"nf4\"\n",
    "        use_nested_quant = False\n",
    "        compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=use_4bit,\n",
    "            bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "            bnb_4bit_compute_dtype=compute_dtype,\n",
    "            bnb_4bit_use_double_quant=use_nested_quant,\n",
    "        )\n",
    "\n",
    "        # --- Load Model ---\n",
    "        device_map = {\"\": 0}\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=device_map,\n",
    "            token=None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        model.config.use_cache = False\n",
    "        model.config.pretraining_tp = 1 # May not be needed after loading but often set\n",
    "\n",
    "        # ===> PREPARE MODEL FOR QLoRA + GRADIENT CHECKPOINTING <===\n",
    "        # This is crucial for compatibility\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        print(\"Model prepared for k-bit training.\")\n",
    "\n",
    "        # --- LoRA Configuration (Using wandb.config) ---\n",
    "        peft_config = LoraConfig(\n",
    "            lora_alpha=config.lora_alpha,\n",
    "            lora_dropout=config.lora_dropout,\n",
    "            r=config.lora_r,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "            target_modules=[\n",
    "                \"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # --- Apply PEFT ---\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "        # ===> EXPLICITLY ENABLE GRADIENT CHECKPOINTING ON PEFT MODEL <===\n",
    "        # Do this AFTER prepare_model... and get_peft_model\n",
    "        # Keep this even if using Trainer's argument (redundancy often needed here)\n",
    "        model.gradient_checkpointing_enable()\n",
    "        print(\"Gradient checkpointing enabled on PEFT model.\")\n",
    "\n",
    "        # --- Tokenize Dataset (using global tokenizer and function) ---\n",
    "        tokenized_dataset = formatted_dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=[\"text\"]\n",
    "        )\n",
    "\n",
    "        # --- Training Arguments Configuration (Using wandb.config) ---\n",
    "        output_dir = os.path.join(wandb.run.dir, \"results\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        training_arguments = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=1,\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "            optim=\"paged_adamw_32bit\",\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=100,\n",
    "            logging_steps=10,\n",
    "            learning_rate=config.learning_rate,\n",
    "            weight_decay=config.weight_decay,\n",
    "            fp16=False, # bf16 is generally preferred if available\n",
    "            bf16=torch.cuda.is_bf16_supported(),\n",
    "            max_grad_norm=0.3,\n",
    "            max_steps=-1,\n",
    "            warmup_ratio=0.03,\n",
    "            group_by_length=True,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            report_to=\"wandb\",\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=100,\n",
    "            per_device_eval_batch_size=1,\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            # ===> KEEP THESE TRAINER ARGS <===\n",
    "            gradient_checkpointing=True,\n",
    "             # Specify use_reentrant=False for the newer implementation\n",
    "            gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "        )\n",
    "\n",
    "        # --- Data Collator ---\n",
    "        data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "        # --- Initialize Trainer ---\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_arguments,\n",
    "            train_dataset=tokenized_dataset[\"train\"],\n",
    "            eval_dataset=tokenized_dataset[\"validation\"],\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "        # --- Start Training ---\n",
    "        print(f\"Starting training for sweep run: {run.id}\")\n",
    "        trainer.train()\n",
    "        print(f\"Training finished for sweep run: {run.id}\")\n",
    "\n",
    "        # Clean up memory\n",
    "        del model\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during sweep run: {e}\")\n",
    "        # Log error to W&B\n",
    "        if run:\n",
    "             run.log({\"error\": str(e)})\n",
    "             # Optionally print traceback\n",
    "             import traceback\n",
    "             traceback.print_exc() # Print full traceback to see where it originates\n",
    "        # raise e # Reraise if you want the agent to stop on error\n",
    "    finally:\n",
    "        # Ensure W&B run is finished even if errors occur\n",
    "        if run:\n",
    "            print(f\"Finishing W&B run: {run.id}\")\n",
    "            run.finish()\n",
    "        print(\"Cleaning up CUDA cache.\")\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run the W&B Sweep Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting W&B agent for sweep ID: kk2hcae9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tig02dc9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_accumulation_steps: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00034890009200157126\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_dropout: 0.06947765483463827\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_r: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/DSPRO2/notebooks/wandb/run-20250501_133606-tig02dc9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/tig02dc9' target=\"_blank\">easy-sweep-1</a></strong> to <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/kk2hcae9' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/kk2hcae9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/kk2hcae9' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/kk2hcae9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/tig02dc9' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/tig02dc9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a339ebe8824ac6b5e73f9b28ad82fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prepared for k-bit training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 65,273,856 || all params: 6,986,994,560 || trainable%: 0.9342\n",
      "Gradient checkpointing enabled on PEFT model.\n",
      "Starting training for sweep run: tig02dc9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "/opt/conda/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1201' max='1217' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1201/1217 10:02:44 < 08:02, 0.03 it/s, Epoch 0.99/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.573700</td>\n",
       "      <td>1.776150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.429000</td>\n",
       "      <td>1.729266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.425800</td>\n",
       "      <td>1.681134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.433600</td>\n",
       "      <td>1.676396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.413100</td>\n",
       "      <td>1.641104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.473900</td>\n",
       "      <td>1.612625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.373000</td>\n",
       "      <td>1.598969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.370600</td>\n",
       "      <td>1.566399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.310300</td>\n",
       "      <td>1.554614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.267200</td>\n",
       "      <td>1.545612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.322900</td>\n",
       "      <td>1.542712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='793' max='1221' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 793/1221 11:58 < 06:28, 1.10 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This will run trials based on the sweep config\n",
    "# count=N specifies how many trials to run. Set to None to run indefinitely (or until stopped).\n",
    "# Ensure sweep_id is defined from the previous cell\n",
    "try:\n",
    "    # Make sure sweep_id is defined from the previous cell (wandb.sweep())\n",
    "    print(f\"Starting W&B agent for sweep ID: {sweep_id}\")\n",
    "    wandb.agent(sweep_id, function=train_sweep, count=10) # Run 10 trials for example\n",
    "    print(\"W&B agent finished.\")\n",
    "except NameError:\n",
    "    print(\"Error: sweep_id is not defined. Please run the sweep initialization cell first.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while running the W&B agent: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Template Code to Continue Training a Specific Run Later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%script false\n",
    "# --- Resume Configuration ---\n",
    "RUN_ID_TO_RESUME = \"abcdef12\" # <--- REPLACE with the actual Run ID from W&B\n",
    "# Find the best checkpoint saved by the run. Often under wandb/sweep-<id>/run-<id>/files/results/checkpoint-XXX\n",
    "# Or if load_best_model_at_end=True, the final saved model in `output_dir` is the best one.\n",
    "CHECKPOINT_PATH_TO_RESUME = \"wandb/sweep-XXXX/run-YYYY/files/results/checkpoint-ZZZ\" # <--- REPLACE with the full path\n",
    "PROJECT_NAME = \"falcon-commonsenseqa-sweep\" # Should match the project used for the sweep\n",
    "ENTITY_NAME = None # Or your W&B entity/username if needed\n",
    "ADDITIONAL_EPOCHS = 2 # How many *more* epochs to train\n",
    "\n",
    "# --- Need Hyperparameters from the Resumed Run ---\n",
    "# You MUST use the same hyperparameters (LoRA config, quantization, etc.)\n",
    "# as the run you are resuming. Fetch these from the W&B run's config.\n",
    "# Example (FETCH THESE VALUES FROM THE W&B RUN DASHBOARD for run RUN_ID_TO_RESUME):\n",
    "print(f\"Fetching config for run: {RUN_ID_TO_RESUME}\")\n",
    "# Use W&B API to get the config of the run to resume (safer than manual copy-paste)\n",
    "resumed_run_api = wandb.Api().run(f\"{ENTITY_NAME or wandb.Api().default_entity}/{PROJECT_NAME}/{RUN_ID_TO_RESUME}\")\n",
    "cfg = resumed_run_api.config\n",
    "print(f\"Fetched Config: {cfg}\")\n",
    "\n",
    "RESUMED_LORA_R = cfg.get(\"lora_r\", 64) # Provide default if key missing\n",
    "RESUMED_LORA_ALPHA = cfg.get(\"lora_alpha\", 16)\n",
    "RESUMED_LORA_DROPOUT = cfg.get(\"lora_dropout\", 0.1)\n",
    "RESUMED_LR = cfg.get(\"learning_rate\", 2e-4)\n",
    "RESUMED_WEIGHT_DECAY = cfg.get(\"weight_decay\", 0.001)\n",
    "RESUMED_GRAD_ACCUM_STEPS = cfg.get(\"gradient_accumulation_steps\", 8)\n",
    "# ... fetch any other tuned parameters used in peft_config or training_args ...\n",
    "\n",
    "# --- Imports (Redundant if run in same session, but good practice) ---\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n",
    "    TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, PeftModel # No get_peft_model needed when loading trained adapter\n",
    "from datasets import load_dataset # Reload dataset if needed\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "# --- W&B Init for Resuming ---\n",
    "print(f\"Attempting to resume W&B run: {RUN_ID_TO_RESUME}\")\n",
    "wandb.login() # Ensure logged in\n",
    "resumed_run = wandb.init(\n",
    "    project=PROJECT_NAME,\n",
    "    id=RUN_ID_TO_RESUME,\n",
    "    resume=\"must\" # Tell W&B to resume this specific run\n",
    ")\n",
    "print(f\"Successfully resumed W&B run: {resumed_run.id} at {resumed_run.path}\")\n",
    "\n",
    "# --- Reload Model and Tokenizer (as done in sweep function) ---\n",
    "model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    token=None,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "base_model.config.use_cache = False # Important for training\n",
    "base_model.config.pretraining_tp = 1\n",
    "\n",
    "# --- Load PEFT Model from Checkpoint ---\n",
    "# The checkpoint should contain the adapter weights and config.\n",
    "print(f\"Loading PEFT model from checkpoint: {CHECKPOINT_PATH_TO_RESUME}\")\n",
    "# is_trainable=True ensures LoRA layers are unfrozen for continued training\n",
    "model = PeftModel.from_pretrained(base_model, CHECKPOINT_PATH_TO_RESUME, is_trainable=True)\n",
    "model.gradient_checkpointing_enable() # Re-enable if needed\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# --- Load Tokenizer (from checkpoint or original) ---\n",
    "# Loading from checkpoint ensures consistency if tokenizer was modified/saved with adapter\n",
    "print(f\"Loading tokenizer from checkpoint: {CHECKPOINT_PATH_TO_RESUME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_PATH_TO_RESUME, token=None)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "# --- Reload and Tokenize Data (ensure it's the same as before) ---\n",
    "print(\"Loading and preprocessing dataset...\")\n",
    "dataset = load_dataset(\"tau/commonsense_qa\")\n",
    "# Redefine or import formatting/tokenization funcs if not in scope\n",
    "# (Assuming format_prompt and tokenize_function are defined as before)\n",
    "formatted_dataset = dataset.map(format_prompt, remove_columns=list(dataset['train'].features))\n",
    "max_sequence_length = 256 # Same as before\n",
    "tokenized_dataset = formatted_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "print(\"Dataset ready.\")\n",
    "\n",
    "# --- Define Training Arguments for Resuming ---\n",
    "# Use a subdirectory within the W&B run's directory for new checkpoints\n",
    "resume_output_dir = os.path.join(resumed_run.dir, \"resume_results\")\n",
    "os.makedirs(resume_output_dir, exist_ok=True)\n",
    "print(f\"Resume output directory: {resume_output_dir}\")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=resume_output_dir, # Save new checkpoints here\n",
    "    num_train_epochs=ADDITIONAL_EPOCHS, # Train for more epochs\n",
    "    per_device_train_batch_size=1, # Match original run if possible, check cfg\n",
    "    gradient_accumulation_steps=RESUMED_GRAD_ACCUM_STEPS, # MUST match resumed run config\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100, # Continue saving checkpoints\n",
    "    logging_steps=10,\n",
    "    learning_rate=RESUMED_LR, # Continue with the same LR or decay schedule\n",
    "    weight_decay=RESUMED_WEIGHT_DECAY, # Match resumed run config\n",
    "    fp16=False,\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1, # Train for specified epochs\n",
    "    warmup_ratio=0.03, # This might restart warmup, consider setting warmup_steps=0 if resuming after warmup\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\", # Match resumed run config\n",
    "    report_to=\"wandb\", # Continue reporting to the same run\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    per_device_eval_batch_size=1,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True, # Load the best model *within this resumed training phase*\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    gradient_checkpointing=True, # Re-enable if needed\n",
    ")\n",
    "\n",
    "# --- Data Collator ---\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# --- Initialize Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# --- Resume Training ---\n",
    "print(f\"Resuming training from checkpoint: {CHECKPOINT_PATH_TO_RESUME}\")\n",
    "# The `resume_from_checkpoint` argument tells Trainer to load optimizer, scheduler, etc. state\n",
    "train_result = trainer.train(resume_from_checkpoint=CHECKPOINT_PATH_TO_RESUME)\n",
    "\n",
    "print(\"Resumed training finished.\")\n",
    "# Save the final model state after this resumed training phase\n",
    "final_save_path = os.path.join(resume_output_dir, \"final_model_after_resume\")\n",
    "trainer.save_model(final_save_path)\n",
    "print(f\"Final model saved to: {final_save_path}\")\n",
    "trainer.log_metrics(\"resume_train\", train_result.metrics)\n",
    "trainer.save_metrics(\"resume_train\", train_result.metrics)\n",
    "\n",
    "# --- Finish W&B Run ---\n",
    "resumed_run.finish()\n",
    "print(f\"Finished and closed W&B run: {resumed_run.id}\")\n",
    "\n",
    "# Clean up memory\n",
    "del model\n",
    "del base_model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "print(\"CUDA cache cleared after resuming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference Example (After Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig # Added for generation parameters\n",
    ")\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import os # For path joining\n",
    "\n",
    "# --- Configuration ---\n",
    "base_model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "# ===> IMPORTANT: UPDATE THIS PATH <===\n",
    "# Point this to the checkpoint or final saved adapter directory you want to use for inference\n",
    "# Example from sweep: \"wandb/sweep-SWEEPID/run-RUNID/files/results/checkpoint-XXX\"\n",
    "# Example from resume: \"wandb/sweep-SWEEPID/run-RUNID/files/resume_results/final_model_after_resume\"\n",
    "# Example from original non-sweep run: \"./results_llama2_7b_commonsenseqa/final_adapter\"\n",
    "adapter_model_dir = \"./results_llama2_7b_commonsenseqa/final_adapter\" # <--- REPLACE Appropriately\n",
    "\n",
    "if not os.path.isdir(adapter_model_dir):\n",
    "     print(f\"ERROR: Adapter directory not found: {adapter_model_dir}\")\n",
    "     # Add logic to stop or raise error\n",
    "     # raise FileNotFoundError(f\"Adapter directory not found: {adapter_model_dir}\")\n",
    "\n",
    "\n",
    "# --- Reload Quantization Config (Must match training) ---\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "device_map = {\"\": 0} # Load model on default GPU\n",
    "\n",
    "# --- Load Base Model ---\n",
    "print(f\"Loading base model: {base_model_name}\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    # low_cpu_mem_usage=True, # Can help if CPU RAM is limited\n",
    "    return_dict=True,\n",
    "    torch_dtype=compute_dtype, # Use compute_dtype\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True if \"falcon\" in base_model_name else False, # Needed for Falcon\n",
    "    token=None # Ensure no token is used for open models\n",
    ")\n",
    "print(\"Base model loaded.\")\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "# Load the tokenizer saved WITH THE ADAPTER (ensures consistency)\n",
    "print(f\"Loading tokenizer from adapter directory: {adapter_model_dir}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_model_dir, token=None)\n",
    "# Ensure padding token is set correctly (usually EOS for these models)\n",
    "if tokenizer.pad_token is None:\n",
    "     tokenizer.pad_token = tokenizer.eos_token\n",
    "     tokenizer.padding_side = \"right\" # Important for generation\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "# --- Load PEFT Adapter ---\n",
    "print(f\"Loading PEFT adapter from: {adapter_model_dir}\")\n",
    "# Load the LoRA adapter onto the base model\n",
    "model = PeftModel.from_pretrained(base_model, adapter_model_dir)\n",
    "print(\"PEFT adapter loaded.\")\n",
    "\n",
    "# --- Merge Adapter (Optional, for faster inference but uses more RAM initially) ---\n",
    "# print(\"Merging adapter weights...\")\n",
    "# model = model.merge_and_unload() # This replaces the model object\n",
    "# print(\"Adapter merged.\")\n",
    "\n",
    "# --- Set to Evaluation Mode ---\n",
    "model.eval()\n",
    "print(\"Model set to evaluation mode.\")\n",
    "\n",
    "# --- Load Original Dataset (for picking a sample) ---\n",
    "print(\"Loading original CommonsenseQA dataset...\")\n",
    "original_dataset = load_dataset(\"tau/commonsense_qa\")\n",
    "validation_data = original_dataset['validation']\n",
    "print(\"Dataset loaded.\")\n",
    "\n",
    "# --- Select a Random Validation Sample ---\n",
    "random_index = random.randint(0, len(validation_data) - 1)\n",
    "sample = validation_data[random_index]\n",
    "\n",
    "# --- Prepare Prompt for Inference (WITHOUT the answer) ---\n",
    "question = sample['question']\n",
    "choices_text = sample['choices']['text']\n",
    "choices_labels = sample['choices']['label']\n",
    "true_answer_key = sample['answerKey']\n",
    "\n",
    "# Format prompt exactly as used in training, but stop before the answer\n",
    "inference_prompt = f\"### Question:\\n{question}\\n\\n### Choices:\\n\"\n",
    "for label, text in zip(choices_labels, choices_text):\n",
    "    inference_prompt += f\"{label}) {text}\\n\"\n",
    "inference_prompt += f\"\\n### Answer:\\n\" # Model generates what comes next\n",
    "\n",
    "# --- Tokenize the Inference Prompt ---\n",
    "device = model.device # Get the device the model is on\n",
    "inputs = tokenizer(inference_prompt, return_tensors=\"pt\", padding=False).to(device)\n",
    "\n",
    "# --- Generate the Answer ---\n",
    "print(\"\\n--- Running Inference ---\")\n",
    "print(f\"Sample Index: {random_index}\")\n",
    "print(\"\\nInput Prompt Sent to Model:\")\n",
    "print(\"---------------------------\")\n",
    "print(inference_prompt)\n",
    "print(\"---------------------------\")\n",
    "print(f\"Actual Answer Key: {true_answer_key}\")\n",
    "print(\"\\nGenerating...\")\n",
    "\n",
    "# Configuration for generation\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=5,       # Generate only a few tokens (A, B, C, D, E + maybe newline/EOS)\n",
    "    temperature=0.1,        # Low temperature for deterministic output\n",
    "    top_p=0.9,              # Can adjust, but low temp is often enough\n",
    "    do_sample=False,        # Use greedy decoding (most likely token)\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    # repetition_penalty=1.1 # Optional: discourage repetition\n",
    ")\n",
    "\n",
    "with torch.no_grad(): # Disable gradient calculation for inference\n",
    "    outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "\n",
    "# --- Decode and Display Results ---\n",
    "# Decode only the newly generated tokens (slice the output tensor)\n",
    "# Ensure slicing accounts for the prompt length correctly\n",
    "input_token_len = inputs['input_ids'].shape[1]\n",
    "generated_token_ids = outputs[0][input_token_len:]\n",
    "generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n--- Results ---\")\n",
    "print(f\"Raw Generated Text: '{generated_text}'\")\n",
    "\n",
    "# Attempt to parse the prediction\n",
    "predicted_answer = generated_text.strip().upper() # Remove whitespace, uppercase\n",
    "parsed_key = None\n",
    "if predicted_answer and predicted_answer[0] in ['A', 'B', 'C', 'D', 'E']:\n",
    "     parsed_key = predicted_answer[0]\n",
    "     print(f\"Parsed Predicted Key: {parsed_key}\")\n",
    "     if parsed_key == true_answer_key:\n",
    "         print(\"Outcome: CORRECT\")\n",
    "     else:\n",
    "         print(\"Outcome: INCORRECT\")\n",
    "else:\n",
    "     print(f\"Outcome: Could not parse a valid key (A-E) from generation: '{predicted_answer}'\")\n",
    "\n",
    "# (Optional) Clean up GPU memory if running multiple inferences\n",
    "# del model\n",
    "# del base_model\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation on Validation Set (After Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: tiiuae/falcon-7b-instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419aabc0114e47e5b2cb0e8f2342f22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading PEFT adapter from: ./wandb/run-20250501_133606-tig02dc9/files/results/checkpoint-1200\n",
      "Model ready for evaluation.\n",
      "Loading dataset split: validation\n",
      "Running evaluation on 1221 samples from 'validation' split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5472c930c774d6e8f77ba0f29eee384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1221 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loop finished.\n",
      "Total parse errors: 0\n",
      "\n",
      "--- Evaluation Metrics ---\n",
      "Split Evaluated:        validation\n",
      "Total Samples:          1221\n",
      "Successfully Parsed:    1221 (100.0% of total, excluding invalid true labels)\n",
      "Accuracy (overall):     0.6470 (Correct predictions / Total samples)\n",
      "Precision (macro, parsed only): 0.6466\n",
      "Recall (macro, parsed only):    0.6466\n",
      "F1 Score (macro, parsed only):  0.6465\n",
      "W&B run not active. Metrics not logged.\n",
      "CUDA cache cleared after evaluation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm.notebook import tqdm # Progress bar\n",
    "import numpy as np\n",
    "import wandb\n",
    "import os # For path joining\n",
    "\n",
    "# --- Configuration (Ensure these match your trained model!) ---\n",
    "base_model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "# ===> IMPORTANT: UPDATE THIS PATH <===\n",
    "# Point this to the checkpoint or final saved adapter directory you want to evaluate\n",
    "adapter_model_dir = \"./wandb/run-20250501_133606-tig02dc9/files/results/checkpoint-1200\"\n",
    "\n",
    "if not os.path.isdir(adapter_model_dir):\n",
    "     print(f\"ERROR: Adapter directory not found: {adapter_model_dir}\")\n",
    "     # Add logic to stop or raise error\n",
    "     # raise FileNotFoundError(f\"Adapter directory not found: {adapter_model_dir}\")\n",
    "\n",
    "\n",
    "dataset_name = \"tau/commonsense_qa\"\n",
    "split_to_evaluate = \"validation\" # Or \"test\" if you want final test metrics\n",
    "# Optional: Initialize W&B if you want to log evaluation metrics to a specific run\n",
    "# EVAL_RUN_ID = \"abcdef12\" # ID of the run you want to associate these metrics with\n",
    "# wandb.init(project=\"falcon-commonsenseqa-sweep\", id=EVAL_RUN_ID, resume=\"allow\")\n",
    "\n",
    "\n",
    "# --- Reload Quantization Config (Must match training) ---\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# --- Load Base Model & Tokenizer ---\n",
    "print(f\"Loading base model: {base_model_name}\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    return_dict=True,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True if \"falcon\" in base_model_name else False,\n",
    "    token=None\n",
    ")\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_model_dir, token=None) # Load tokenizer saved with adapter\n",
    "if tokenizer.pad_token is None:\n",
    "     tokenizer.pad_token = tokenizer.eos_token\n",
    "     tokenizer.padding_side = \"right\" # Use right padding for generation\n",
    "\n",
    "# --- Load PEFT Adapter ---\n",
    "print(f\"Loading PEFT adapter from: {adapter_model_dir}\")\n",
    "model = PeftModel.from_pretrained(base_model, adapter_model_dir)\n",
    "model.eval() # Set to evaluation mode\n",
    "device = model.device\n",
    "print(\"Model ready for evaluation.\")\n",
    "\n",
    "# --- Load Dataset Split ---\n",
    "print(f\"Loading dataset split: {split_to_evaluate}\")\n",
    "eval_dataset = load_dataset(dataset_name, split=split_to_evaluate)\n",
    "\n",
    "# --- Prepare for Evaluation ---\n",
    "label_map = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
    "reverse_label_map = {v: k for k, v in label_map.items()} # For debugging\n",
    "y_true_eval = []\n",
    "y_pred_eval = []\n",
    "y_pred_parsed_keys = [] # Store parsed keys (A, B, C, D, E) or None\n",
    "y_true_keys = [] # Store true keys\n",
    "\n",
    "# Generation config (deterministic for evaluation)\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=5, # Enough for the letter + maybe EOS/newline\n",
    "    temperature=0.1,  # Low temp\n",
    "    do_sample=False,  # Greedy decoding\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# --- Evaluation Loop ---\n",
    "print(f\"Running evaluation on {len(eval_dataset)} samples from '{split_to_evaluate}' split...\")\n",
    "num_parse_errors = 0\n",
    "for i in tqdm(range(len(eval_dataset))):\n",
    "    sample = eval_dataset[i]\n",
    "    question = sample['question']\n",
    "    choices_text = sample['choices']['text']\n",
    "    choices_labels = sample['choices']['label']\n",
    "    true_answer_key = sample['answerKey']\n",
    "    true_label_numeric = label_map.get(true_answer_key, -1) # Handle potential invalid keys in data\n",
    "\n",
    "    y_true_keys.append(true_answer_key)\n",
    "    y_true_eval.append(true_label_numeric)\n",
    "\n",
    "    # Format prompt\n",
    "    inference_prompt = f\"### Question:\\n{question}\\n\\n### Choices:\\n\"\n",
    "    for label, text in zip(choices_labels, choices_text):\n",
    "        inference_prompt += f\"{label}) {text}\\n\"\n",
    "    inference_prompt += f\"\\n### Answer:\\n\"\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(inference_prompt, return_tensors=\"pt\", padding=False).to(device)\n",
    "\n",
    "    # Generate\n",
    "    predicted_key_numeric = -1 # Default to -1 for parse failure\n",
    "    predicted_parsed_key = None\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "        # Decode generated part\n",
    "        input_token_len = inputs['input_ids'].shape[1]\n",
    "        generated_token_ids = outputs[0][input_token_len:]\n",
    "        generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Parse prediction\n",
    "        parsed_pred = generated_text.strip().upper()\n",
    "        if parsed_pred and parsed_pred[0] in label_map:\n",
    "            predicted_parsed_key = parsed_pred[0]\n",
    "            predicted_key_numeric = label_map[predicted_parsed_key]\n",
    "        else:\n",
    "            num_parse_errors += 1\n",
    "            # Optional: Log the failed parse\n",
    "            # print(f\"Parse Error (Index {i}): Prompt:\\n{inference_prompt}\\nGenerated: '{generated_text}' Parsed: '{parsed_pred}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during generation/parsing for index {i}: {e}\")\n",
    "        num_parse_errors += 1\n",
    "        # Keep predicted_key_numeric as -1 and predicted_parsed_key as None\n",
    "\n",
    "    y_pred_eval.append(predicted_key_numeric)\n",
    "    y_pred_parsed_keys.append(predicted_parsed_key)\n",
    "\n",
    "\n",
    "print(\"Evaluation loop finished.\")\n",
    "print(f\"Total parse errors: {num_parse_errors}\")\n",
    "\n",
    "# --- Calculate Metrics ---\n",
    "# Overall accuracy includes parse failures (counted as incorrect)\n",
    "correct_predictions = sum(1 for true, pred in zip(y_true_eval, y_pred_eval) if true == pred and true != -1)\n",
    "total_samples = len(y_true_eval)\n",
    "accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "# For Precision, Recall, F1, only consider samples where parsing succeeded *and* true label was valid\n",
    "valid_indices = [i for i, (p, t) in enumerate(zip(y_pred_eval, y_true_eval)) if p != -1 and t != -1]\n",
    "\n",
    "if len(valid_indices) > 0:\n",
    "    filtered_y_true = [y_true_eval[i] for i in valid_indices]\n",
    "    filtered_y_pred = [y_pred_eval[i] for i in valid_indices]\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        filtered_y_true,\n",
    "        filtered_y_pred,\n",
    "        average='macro', # Average metrics across classes (A, B, C, D, E)\n",
    "        zero_division=0   # Set metric to 0 if no predictions/true labels for a class\n",
    "    )\n",
    "    num_parsed = len(valid_indices)\n",
    "else:\n",
    "    print(\"Warning: Could not parse any valid predictions (A-E), or no valid true labels found. Precision/Recall/F1 will be 0.\")\n",
    "    precision, recall, f1 = 0.0, 0.0, 0.0\n",
    "    num_parsed = 0\n",
    "\n",
    "\n",
    "# --- Print Results ---\n",
    "print(\"\\n--- Evaluation Metrics ---\")\n",
    "print(f\"Split Evaluated:        {split_to_evaluate}\")\n",
    "print(f\"Total Samples:          {total_samples}\")\n",
    "print(f\"Successfully Parsed:    {num_parsed} ({num_parsed/total_samples:.1%} of total, excluding invalid true labels)\")\n",
    "print(f\"Accuracy (overall):     {accuracy:.4f} (Correct predictions / Total samples)\")\n",
    "print(f\"Precision (macro, parsed only): {precision:.4f}\")\n",
    "print(f\"Recall (macro, parsed only):    {recall:.4f}\")\n",
    "print(f\"F1 Score (macro, parsed only):  {f1:.4f}\")\n",
    "\n",
    "# --- Log to W&B if initialized ---\n",
    "if wandb.run:\n",
    "    wandb.log({\n",
    "        f\"{split_to_evaluate}_accuracy\": accuracy,\n",
    "        f\"{split_to_evaluate}_precision_macro\": precision,\n",
    "        f\"{split_to_evaluate}_recall_macro\": recall,\n",
    "        f\"{split_to_evaluate}_f1_macro\": f1,\n",
    "        f\"{split_to_evaluate}_parsed_count\": num_parsed,\n",
    "        f\"{split_to_evaluate}_total_count\": total_samples,\n",
    "        f\"{split_to_evaluate}_parse_errors\": num_parse_errors\n",
    "    })\n",
    "    print(f\"Metrics logged to W&B run: {wandb.run.path}\")\n",
    "    # Optional: Finish the run if it was only for evaluation\n",
    "    # wandb.finish()\n",
    "else:\n",
    "    print(\"W&B run not active. Metrics not logged.\")\n",
    "\n",
    "\n",
    "# (Optional) Clean up GPU memory\n",
    "del model\n",
    "del base_model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"CUDA cache cleared after evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
