{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 datasets evaluate sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdanielbetschart\u001b[0m (\u001b[33mdanielbetschart-hochschule-luzern\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dabe/Projects/HSLU/S4/DSPRO2/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 9741\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 1221\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 1140\n",
      "    })\n",
      "})\n",
      "\n",
      "Example Train instance:\n",
      "{'id': '075e483d21c29a511267ef62bedc0461', 'question': 'The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?', 'question_concept': 'punishing', 'choices': {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['ignore', 'enforce', 'authoritarian', 'yell at', 'avoid']}, 'answerKey': 'A'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"tau/commonsense_qa\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "print(\"Dataset loaded:\")\n",
    "print(dataset)\n",
    "print(\"\\nExample Train instance:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration & Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example Formatted Prompt (for training):\n",
      "### Question:\n",
      "The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
      "\n",
      "### Choices:\n",
      "A) ignore\n",
      "B) enforce\n",
      "C) authoritarian\n",
      "D) yell at\n",
      "E) avoid\n",
      "\n",
      "### Answer:\n",
      "A\n",
      "Formatting and tokenizer setup complete. Tokenization will occur within sweep runs.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer # Need tokenizer info early\n",
    "\n",
    "# --- Define formatting function ---\n",
    "def format_prompt(example):\n",
    "    question = example['question']\n",
    "    choices_text = example['choices']['text']\n",
    "    choices_labels = example['choices']['label']\n",
    "    answer_key = example['answerKey']\n",
    "\n",
    "    prompt = f\"### Question:\\n{question}\\n\\n### Choices:\\n\"\n",
    "    for label, text in zip(choices_labels, choices_text):\n",
    "        prompt += f\"{label}) {text}\\n\"\n",
    "    prompt += f\"\\n### Answer:\\n{answer_key}\"\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# Apply formatting\n",
    "formatted_dataset = dataset.map(format_prompt, remove_columns=list(dataset['train'].features))\n",
    "print(\"\\nExample Formatted Prompt (for training):\")\n",
    "print(formatted_dataset['train'][0]['text'])\n",
    "\n",
    "# --- Define Tokenization (but don't run tokenization globally yet) ---\n",
    "# We need the tokenizer object available, but tokenization will happen inside the sweep function\n",
    "base_model_name_for_tokenizer = \"tiiuae/falcon-7b-instruct\" # Needs to match model used in sweep\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name_for_tokenizer, token=None)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "max_sequence_length = 256 # Define max length here\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Use the globally defined tokenizer and max_sequence_length\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=max_sequence_length,\n",
    "    )\n",
    "\n",
    "# Note: We map the tokenize_function inside the train_sweep function later\n",
    "print(\"Formatting and tokenizer setup complete. Tokenization will occur within sweep runs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ad4xa5vx\n",
      "Sweep URL: https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx\n",
      "Sweep ID: ad4xa5vx\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes', # Or 'random', 'grid'\n",
    "    'metric': {\n",
    "      'name': 'eval/loss', # Optimize for lowest validation loss\n",
    "      'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 1e-5,\n",
    "            'max': 5e-4\n",
    "        },\n",
    "        'lora_r': {\n",
    "            'values': [16, 32, 64]\n",
    "        },\n",
    "        'lora_alpha': {\n",
    "             # Often set relative to r, e.g., 2*r, but let's try independent values\n",
    "            'values': [16, 32, 64]\n",
    "        },\n",
    "        'lora_dropout': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.05,\n",
    "            'max': 0.2\n",
    "        },\n",
    "        'gradient_accumulation_steps': {\n",
    "            'values': [4, 8]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [0.0, 0.001, 0.01]\n",
    "        }\n",
    "        # Add other parameters like per_device_train_batch_size if desired\n",
    "        # 'per_device_train_batch_size': {'values': [1, 2]}\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Initialize the Sweep ---\n",
    "# Make sure you are logged into W&B (`wandb login`)\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"falcon-commonsenseqa-sweep\") # Choose a project name\n",
    "print(f\"Sweep ID: {sweep_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing - Format Data as Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    # AutoTokenizer loaded globally\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import os\n",
    "import wandb # Make sure wandb is imported here too\n",
    "\n",
    "# Ensure global variables (tokenizer, formatted_dataset, tokenize_function, max_sequence_length)\n",
    "# are accessible within this function's scope. If running as a script, define them above or pass as args.\n",
    "\n",
    "def train_sweep():\n",
    "    run = None # Define run here to ensure it's accessible in finally block\n",
    "    try:\n",
    "        # Initialize W&B run for this specific sweep trial\n",
    "        # W&B agent automatically sets config from sweep if run within `wandb.agent`\n",
    "        # If running this function standalone for testing, you might need to manually init with a config\n",
    "        run = wandb.init()\n",
    "        if run is None:\n",
    "             # Fallback for testing outside agent - provide dummy config\n",
    "             print(\"WARNING: wandb.init() returned None. Running with default test config.\")\n",
    "             # Define a sample config for testing if needed\n",
    "             class DummyConfig:\n",
    "                 learning_rate = 2e-4\n",
    "                 lora_r = 64\n",
    "                 lora_alpha = 16\n",
    "                 lora_dropout = 0.1\n",
    "                 gradient_accumulation_steps = 8\n",
    "                 weight_decay = 0.001\n",
    "             config = DummyConfig()\n",
    "             # Manually init wandb for testing if not using agent\n",
    "             run = wandb.init(project=\"falcon-sweep-test\", config=vars(config))\n",
    "        else:\n",
    "             config = wandb.config # Access hyperparameters for this run provided by the agent\n",
    "\n",
    "        # --- Model Configuration ---\n",
    "        model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "        # --- QLoRA Configuration ---\n",
    "        use_4bit = True\n",
    "        bnb_4bit_compute_dtype = \"float16\"\n",
    "        bnb_4bit_quant_type = \"nf4\"\n",
    "        use_nested_quant = False\n",
    "        compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=use_4bit,\n",
    "            bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "            bnb_4bit_compute_dtype=compute_dtype,\n",
    "            bnb_4bit_use_double_quant=use_nested_quant,\n",
    "        )\n",
    "\n",
    "        # --- LoRA Configuration (Using wandb.config) ---\n",
    "        peft_config = LoraConfig(\n",
    "            lora_alpha=config.lora_alpha,\n",
    "            lora_dropout=config.lora_dropout,\n",
    "            r=config.lora_r,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "            target_modules=[\n",
    "                \"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # --- Load Model and Prepare for PEFT ---\n",
    "        device_map = {\"\": 0}\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=device_map,\n",
    "            token=None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        model.config.use_cache = False\n",
    "        model.config.pretraining_tp = 1\n",
    "        # Check if model needs gradients enabled for PEFT\n",
    "        # model.gradient_checkpointing_enable() # Usually needed for memory saving\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "        # --- Tokenize Dataset (using global tokenizer and function) ---\n",
    "        # It's often better to tokenize once per sweep run if memory allows\n",
    "        # Ensure `formatted_dataset` and `tokenize_function` are accessible\n",
    "        tokenized_dataset = formatted_dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=[\"text\"]\n",
    "        )\n",
    "\n",
    "        # --- Training Arguments Configuration (Using wandb.config) ---\n",
    "        # Output dir should be unique per run, wandb handles this by default\n",
    "        # wandb.run.dir gives the directory for the current run\n",
    "        output_dir = os.path.join(wandb.run.dir, \"results\") # Use run dir from wandb.init()\n",
    "        os.makedirs(output_dir, exist_ok=True) # Ensure the directory exists\n",
    "\n",
    "        training_arguments = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=1, # Run for 1 epoch during the sweep\n",
    "            per_device_train_batch_size=1, # Keep low, or tune this: config.per_device_train_batch_size\n",
    "            gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "            optim=\"paged_adamw_32bit\",\n",
    "            save_strategy=\"steps\", # Save based on steps\n",
    "            save_steps=100, # Save checkpoints periodically\n",
    "            logging_steps=10,\n",
    "            learning_rate=config.learning_rate,\n",
    "            weight_decay=config.weight_decay,\n",
    "            fp16=False, # Set based on GPU capability if needed\n",
    "            bf16=torch.cuda.is_bf16_supported(), # Automatically detect bf16 support\n",
    "            max_grad_norm=0.3,\n",
    "            max_steps=-1, # Let num_train_epochs control duration\n",
    "            warmup_ratio=0.03,\n",
    "            group_by_length=True,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            report_to=\"wandb\", # Report metrics to W&B\n",
    "            # --- Evaluation Args ---\n",
    "            evaluation_strategy=\"steps\", # Evaluate during training\n",
    "            eval_steps=100,               # Evaluate every N steps (match save_steps?)\n",
    "            per_device_eval_batch_size=1, # Keep low for memory\n",
    "            save_total_limit=2,           # Keep only the best 2 checkpoints based on metric\n",
    "            load_best_model_at_end=True, # Load the best model based on metric_for_best_model\n",
    "            metric_for_best_model=\"eval_loss\", # Metric to determine the best model\n",
    "            greater_is_better=False,      # Lower eval_loss is better\n",
    "            gradient_checkpointing=True, # Usually beneficial for memory\n",
    "        )\n",
    "\n",
    "        # --- Data Collator ---\n",
    "        # Use the global tokenizer\n",
    "        data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "        # --- Initialize Trainer ---\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_arguments,\n",
    "            train_dataset=tokenized_dataset[\"train\"],\n",
    "            eval_dataset=tokenized_dataset[\"validation\"], # Pass validation set for evaluation\n",
    "            data_collator=data_collator,\n",
    "            # No compute_metrics needed if optimizing for eval/loss\n",
    "        )\n",
    "\n",
    "        # --- Start Training ---\n",
    "        print(f\"Starting training for sweep run: {run.id}\") # Log run ID\n",
    "        trainer.train()\n",
    "        print(f\"Training finished for sweep run: {run.id}\")\n",
    "\n",
    "        # Trainer automatically saves the best model due to `load_best_model_at_end=True`\n",
    "        # You can manually save the final state if needed, but the best checkpoint is usually preferred\n",
    "        # final_adapter_dir = os.path.join(output_dir, \"final_adapter\")\n",
    "        # trainer.save_model(final_adapter_dir) # Saves adapter & tokenizer\n",
    "        # print(f\"Final adapter saved to {final_adapter_dir} for run {run.id}\")\n",
    "\n",
    "        # Final evaluation results are logged automatically by Trainer callback with report_to=\"wandb\"\n",
    "\n",
    "        # Clean up memory\n",
    "        del model\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during sweep run: {e}\")\n",
    "        # Optional: Log error to W&B\n",
    "        if run:\n",
    "             run.log({\"error\": str(e)})\n",
    "        # Reraise the exception to make it visible to the agent/user\n",
    "        # raise e\n",
    "    finally:\n",
    "        # Ensure W&B run is finished even if errors occur\n",
    "        if run:\n",
    "            print(f\"Finishing W&B run: {run.id}\")\n",
    "            run.finish()\n",
    "        print(\"Cleaning up CUDA cache.\")\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run the W&B Sweep Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting W&B agent for sweep ID: ad4xa5vx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b6f0ztac with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_accumulation_steps: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3.691735646163188e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_dropout: 0.1430060029263187\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_r: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dabe/Projects/HSLU/S4/DSPRO2/wandb/run-20250501_143147-b6f0ztac</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/b6f0ztac' target=\"_blank\">clear-sweep-1</a></strong> to <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/b6f0ztac' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/b6f0ztac</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during sweep run: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n",
      "Finishing W&B run: b6f0ztac\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>error</td><td>CUDA is required but...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clear-sweep-1</strong> at: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/b6f0ztac' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/b6f0ztac</a><br> View project at: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250501_143147-b6f0ztac/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up CUDA cache.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xruyhc63 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_accumulation_steps: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00026669717887190767\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_dropout: 0.13884566200507076\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_r: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dabe/Projects/HSLU/S4/DSPRO2/wandb/run-20250501_143153-xruyhc63</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/xruyhc63' target=\"_blank\">devout-sweep-2</a></strong> to <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/xruyhc63' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/xruyhc63</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during sweep run: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n",
      "Finishing W&B run: xruyhc63\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>error</td><td>CUDA is required but...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devout-sweep-2</strong> at: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/xruyhc63' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/xruyhc63</a><br> View project at: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250501_143153-xruyhc63/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up CUDA cache.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6dvx1o0x with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_accumulation_steps: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3.630033650724389e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_dropout: 0.15964529260078383\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_r: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dabe/Projects/HSLU/S4/DSPRO2/wandb/run-20250501_143158-6dvx1o0x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/6dvx1o0x' target=\"_blank\">swept-sweep-3</a></strong> to <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/6dvx1o0x' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/6dvx1o0x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during sweep run: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n",
      "Finishing W&B run: 6dvx1o0x\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>error</td><td>CUDA is required but...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swept-sweep-3</strong> at: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/6dvx1o0x' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/6dvx1o0x</a><br> View project at: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250501_143158-6dvx1o0x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up CUDA cache.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: q5wmvki6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_accumulation_steps: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 7.158208644104399e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_dropout: 0.05521280808626469\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_r: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dabe/Projects/HSLU/S4/DSPRO2/wandb/run-20250501_143204-q5wmvki6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/q5wmvki6' target=\"_blank\">lively-sweep-4</a></strong> to <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/q5wmvki6' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/q5wmvki6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during sweep run: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n",
      "Finishing W&B run: q5wmvki6\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>error</td><td>CUDA is required but...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lively-sweep-4</strong> at: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/q5wmvki6' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/q5wmvki6</a><br> View project at: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250501_143204-q5wmvki6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up CUDA cache.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xf0q7b96 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_accumulation_steps: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3.458015301017141e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_dropout: 0.11339701244407176\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_r: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dabe/Projects/HSLU/S4/DSPRO2/wandb/run-20250501_143209-xf0q7b96</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/xf0q7b96' target=\"_blank\">peach-sweep-5</a></strong> to <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/xf0q7b96' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/xf0q7b96</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during sweep run: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n",
      "Finishing W&B run: xf0q7b96\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>error</td><td>CUDA is required but...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peach-sweep-5</strong> at: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/xf0q7b96' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/xf0q7b96</a><br> View project at: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250501_143209-xf0q7b96/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up CUDA cache.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fe1c5dgk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_accumulation_steps: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001303602415589188\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_dropout: 0.05280422387970812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_r: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dabe/Projects/HSLU/S4/DSPRO2/wandb/run-20250501_143216-fe1c5dgk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/fe1c5dgk' target=\"_blank\">hearty-sweep-6</a></strong> to <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/sweeps/ad4xa5vx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/fe1c5dgk' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/fe1c5dgk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during sweep run: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n",
      "Finishing W&B run: fe1c5dgk\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>error</td><td>CUDA is required but...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hearty-sweep-6</strong> at: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/fe1c5dgk' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep/runs/fe1c5dgk</a><br> View project at: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/falcon-commonsenseqa-sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250501_143216-fe1c5dgk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up CUDA cache.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B agent finished.\n"
     ]
    }
   ],
   "source": [
    "# This will run trials based on the sweep config\n",
    "# count=N specifies how many trials to run. Set to None to run indefinitely (or until stopped).\n",
    "# Ensure sweep_id is defined from the previous cell\n",
    "try:\n",
    "    # Make sure sweep_id is defined from the previous cell (wandb.sweep())\n",
    "    print(f\"Starting W&B agent for sweep ID: {sweep_id}\")\n",
    "    wandb.agent(sweep_id, function=train_sweep, count=10) # Run 10 trials for example\n",
    "    print(\"W&B agent finished.\")\n",
    "except NameError:\n",
    "    print(\"Error: sweep_id is not defined. Please run the sweep initialization cell first.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while running the W&B agent: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Template Code to Continue Training a Specific Run Later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%script false\n",
    "# --- Resume Configuration ---\n",
    "RUN_ID_TO_RESUME = \"abcdef12\" # <--- REPLACE with the actual Run ID from W&B\n",
    "# Find the best checkpoint saved by the run. Often under wandb/sweep-<id>/run-<id>/files/results/checkpoint-XXX\n",
    "# Or if load_best_model_at_end=True, the final saved model in `output_dir` is the best one.\n",
    "CHECKPOINT_PATH_TO_RESUME = \"wandb/sweep-XXXX/run-YYYY/files/results/checkpoint-ZZZ\" # <--- REPLACE with the full path\n",
    "PROJECT_NAME = \"falcon-commonsenseqa-sweep\" # Should match the project used for the sweep\n",
    "ENTITY_NAME = None # Or your W&B entity/username if needed\n",
    "ADDITIONAL_EPOCHS = 2 # How many *more* epochs to train\n",
    "\n",
    "# --- Need Hyperparameters from the Resumed Run ---\n",
    "# You MUST use the same hyperparameters (LoRA config, quantization, etc.)\n",
    "# as the run you are resuming. Fetch these from the W&B run's config.\n",
    "# Example (FETCH THESE VALUES FROM THE W&B RUN DASHBOARD for run RUN_ID_TO_RESUME):\n",
    "print(f\"Fetching config for run: {RUN_ID_TO_RESUME}\")\n",
    "# Use W&B API to get the config of the run to resume (safer than manual copy-paste)\n",
    "resumed_run_api = wandb.Api().run(f\"{ENTITY_NAME or wandb.Api().default_entity}/{PROJECT_NAME}/{RUN_ID_TO_RESUME}\")\n",
    "cfg = resumed_run_api.config\n",
    "print(f\"Fetched Config: {cfg}\")\n",
    "\n",
    "RESUMED_LORA_R = cfg.get(\"lora_r\", 64) # Provide default if key missing\n",
    "RESUMED_LORA_ALPHA = cfg.get(\"lora_alpha\", 16)\n",
    "RESUMED_LORA_DROPOUT = cfg.get(\"lora_dropout\", 0.1)\n",
    "RESUMED_LR = cfg.get(\"learning_rate\", 2e-4)\n",
    "RESUMED_WEIGHT_DECAY = cfg.get(\"weight_decay\", 0.001)\n",
    "RESUMED_GRAD_ACCUM_STEPS = cfg.get(\"gradient_accumulation_steps\", 8)\n",
    "# ... fetch any other tuned parameters used in peft_config or training_args ...\n",
    "\n",
    "# --- Imports (Redundant if run in same session, but good practice) ---\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n",
    "    TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, PeftModel # No get_peft_model needed when loading trained adapter\n",
    "from datasets import load_dataset # Reload dataset if needed\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "# --- W&B Init for Resuming ---\n",
    "print(f\"Attempting to resume W&B run: {RUN_ID_TO_RESUME}\")\n",
    "wandb.login() # Ensure logged in\n",
    "resumed_run = wandb.init(\n",
    "    project=PROJECT_NAME,\n",
    "    id=RUN_ID_TO_RESUME,\n",
    "    resume=\"must\" # Tell W&B to resume this specific run\n",
    ")\n",
    "print(f\"Successfully resumed W&B run: {resumed_run.id} at {resumed_run.path}\")\n",
    "\n",
    "# --- Reload Model and Tokenizer (as done in sweep function) ---\n",
    "model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    token=None,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "base_model.config.use_cache = False # Important for training\n",
    "base_model.config.pretraining_tp = 1\n",
    "\n",
    "# --- Load PEFT Model from Checkpoint ---\n",
    "# The checkpoint should contain the adapter weights and config.\n",
    "print(f\"Loading PEFT model from checkpoint: {CHECKPOINT_PATH_TO_RESUME}\")\n",
    "# is_trainable=True ensures LoRA layers are unfrozen for continued training\n",
    "model = PeftModel.from_pretrained(base_model, CHECKPOINT_PATH_TO_RESUME, is_trainable=True)\n",
    "model.gradient_checkpointing_enable() # Re-enable if needed\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# --- Load Tokenizer (from checkpoint or original) ---\n",
    "# Loading from checkpoint ensures consistency if tokenizer was modified/saved with adapter\n",
    "print(f\"Loading tokenizer from checkpoint: {CHECKPOINT_PATH_TO_RESUME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_PATH_TO_RESUME, token=None)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "# --- Reload and Tokenize Data (ensure it's the same as before) ---\n",
    "print(\"Loading and preprocessing dataset...\")\n",
    "dataset = load_dataset(\"tau/commonsense_qa\")\n",
    "# Redefine or import formatting/tokenization funcs if not in scope\n",
    "# (Assuming format_prompt and tokenize_function are defined as before)\n",
    "formatted_dataset = dataset.map(format_prompt, remove_columns=list(dataset['train'].features))\n",
    "max_sequence_length = 256 # Same as before\n",
    "tokenized_dataset = formatted_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "print(\"Dataset ready.\")\n",
    "\n",
    "# --- Define Training Arguments for Resuming ---\n",
    "# Use a subdirectory within the W&B run's directory for new checkpoints\n",
    "resume_output_dir = os.path.join(resumed_run.dir, \"resume_results\")\n",
    "os.makedirs(resume_output_dir, exist_ok=True)\n",
    "print(f\"Resume output directory: {resume_output_dir}\")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=resume_output_dir, # Save new checkpoints here\n",
    "    num_train_epochs=ADDITIONAL_EPOCHS, # Train for more epochs\n",
    "    per_device_train_batch_size=1, # Match original run if possible, check cfg\n",
    "    gradient_accumulation_steps=RESUMED_GRAD_ACCUM_STEPS, # MUST match resumed run config\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100, # Continue saving checkpoints\n",
    "    logging_steps=10,\n",
    "    learning_rate=RESUMED_LR, # Continue with the same LR or decay schedule\n",
    "    weight_decay=RESUMED_WEIGHT_DECAY, # Match resumed run config\n",
    "    fp16=False,\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1, # Train for specified epochs\n",
    "    warmup_ratio=0.03, # This might restart warmup, consider setting warmup_steps=0 if resuming after warmup\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\", # Match resumed run config\n",
    "    report_to=\"wandb\", # Continue reporting to the same run\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    per_device_eval_batch_size=1,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True, # Load the best model *within this resumed training phase*\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    gradient_checkpointing=True, # Re-enable if needed\n",
    ")\n",
    "\n",
    "# --- Data Collator ---\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# --- Initialize Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# --- Resume Training ---\n",
    "print(f\"Resuming training from checkpoint: {CHECKPOINT_PATH_TO_RESUME}\")\n",
    "# The `resume_from_checkpoint` argument tells Trainer to load optimizer, scheduler, etc. state\n",
    "train_result = trainer.train(resume_from_checkpoint=CHECKPOINT_PATH_TO_RESUME)\n",
    "\n",
    "print(\"Resumed training finished.\")\n",
    "# Save the final model state after this resumed training phase\n",
    "final_save_path = os.path.join(resume_output_dir, \"final_model_after_resume\")\n",
    "trainer.save_model(final_save_path)\n",
    "print(f\"Final model saved to: {final_save_path}\")\n",
    "trainer.log_metrics(\"resume_train\", train_result.metrics)\n",
    "trainer.save_metrics(\"resume_train\", train_result.metrics)\n",
    "\n",
    "# --- Finish W&B Run ---\n",
    "resumed_run.finish()\n",
    "print(f\"Finished and closed W&B run: {resumed_run.id}\")\n",
    "\n",
    "# Clean up memory\n",
    "del model\n",
    "del base_model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "print(\"CUDA cache cleared after resuming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference Example (After Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the final PEFT adapter model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving the final PEFT adapter model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39msave_state() \u001b[38;5;66;03m# Save trainer state\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# The PEFT adapter weights are saved in the output_dir checkpoints\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# To save the final adapter separately:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m final_adapter_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/final_adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig # Added for generation parameters\n",
    ")\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import os # For path joining\n",
    "\n",
    "# --- Configuration ---\n",
    "base_model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "# ===> IMPORTANT: UPDATE THIS PATH <===\n",
    "# Point this to the checkpoint or final saved adapter directory you want to use for inference\n",
    "# Example from sweep: \"wandb/sweep-SWEEPID/run-RUNID/files/results/checkpoint-XXX\"\n",
    "# Example from resume: \"wandb/sweep-SWEEPID/run-RUNID/files/resume_results/final_model_after_resume\"\n",
    "# Example from original non-sweep run: \"./results_llama2_7b_commonsenseqa/final_adapter\"\n",
    "adapter_model_dir = \"./results_llama2_7b_commonsenseqa/final_adapter\" # <--- REPLACE Appropriately\n",
    "\n",
    "if not os.path.isdir(adapter_model_dir):\n",
    "     print(f\"ERROR: Adapter directory not found: {adapter_model_dir}\")\n",
    "     # Add logic to stop or raise error\n",
    "     # raise FileNotFoundError(f\"Adapter directory not found: {adapter_model_dir}\")\n",
    "\n",
    "\n",
    "# --- Reload Quantization Config (Must match training) ---\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "device_map = {\"\": 0} # Load model on default GPU\n",
    "\n",
    "# --- Load Base Model ---\n",
    "print(f\"Loading base model: {base_model_name}\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    # low_cpu_mem_usage=True, # Can help if CPU RAM is limited\n",
    "    return_dict=True,\n",
    "    torch_dtype=compute_dtype, # Use compute_dtype\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True if \"falcon\" in base_model_name else False, # Needed for Falcon\n",
    "    token=None # Ensure no token is used for open models\n",
    ")\n",
    "print(\"Base model loaded.\")\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "# Load the tokenizer saved WITH THE ADAPTER (ensures consistency)\n",
    "print(f\"Loading tokenizer from adapter directory: {adapter_model_dir}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_model_dir, token=None)\n",
    "# Ensure padding token is set correctly (usually EOS for these models)\n",
    "if tokenizer.pad_token is None:\n",
    "     tokenizer.pad_token = tokenizer.eos_token\n",
    "     tokenizer.padding_side = \"right\" # Important for generation\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "# --- Load PEFT Adapter ---\n",
    "print(f\"Loading PEFT adapter from: {adapter_model_dir}\")\n",
    "# Load the LoRA adapter onto the base model\n",
    "model = PeftModel.from_pretrained(base_model, adapter_model_dir)\n",
    "print(\"PEFT adapter loaded.\")\n",
    "\n",
    "# --- Merge Adapter (Optional, for faster inference but uses more RAM initially) ---\n",
    "# print(\"Merging adapter weights...\")\n",
    "# model = model.merge_and_unload() # This replaces the model object\n",
    "# print(\"Adapter merged.\")\n",
    "\n",
    "# --- Set to Evaluation Mode ---\n",
    "model.eval()\n",
    "print(\"Model set to evaluation mode.\")\n",
    "\n",
    "# --- Load Original Dataset (for picking a sample) ---\n",
    "print(\"Loading original CommonsenseQA dataset...\")\n",
    "original_dataset = load_dataset(\"tau/commonsense_qa\")\n",
    "validation_data = original_dataset['validation']\n",
    "print(\"Dataset loaded.\")\n",
    "\n",
    "# --- Select a Random Validation Sample ---\n",
    "random_index = random.randint(0, len(validation_data) - 1)\n",
    "sample = validation_data[random_index]\n",
    "\n",
    "# --- Prepare Prompt for Inference (WITHOUT the answer) ---\n",
    "question = sample['question']\n",
    "choices_text = sample['choices']['text']\n",
    "choices_labels = sample['choices']['label']\n",
    "true_answer_key = sample['answerKey']\n",
    "\n",
    "# Format prompt exactly as used in training, but stop before the answer\n",
    "inference_prompt = f\"### Question:\\n{question}\\n\\n### Choices:\\n\"\n",
    "for label, text in zip(choices_labels, choices_text):\n",
    "    inference_prompt += f\"{label}) {text}\\n\"\n",
    "inference_prompt += f\"\\n### Answer:\\n\" # Model generates what comes next\n",
    "\n",
    "# --- Tokenize the Inference Prompt ---\n",
    "device = model.device # Get the device the model is on\n",
    "inputs = tokenizer(inference_prompt, return_tensors=\"pt\", padding=False).to(device)\n",
    "\n",
    "# --- Generate the Answer ---\n",
    "print(\"\\n--- Running Inference ---\")\n",
    "print(f\"Sample Index: {random_index}\")\n",
    "print(\"\\nInput Prompt Sent to Model:\")\n",
    "print(\"---------------------------\")\n",
    "print(inference_prompt)\n",
    "print(\"---------------------------\")\n",
    "print(f\"Actual Answer Key: {true_answer_key}\")\n",
    "print(\"\\nGenerating...\")\n",
    "\n",
    "# Configuration for generation\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=5,       # Generate only a few tokens (A, B, C, D, E + maybe newline/EOS)\n",
    "    temperature=0.1,        # Low temperature for deterministic output\n",
    "    top_p=0.9,              # Can adjust, but low temp is often enough\n",
    "    do_sample=False,        # Use greedy decoding (most likely token)\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    # repetition_penalty=1.1 # Optional: discourage repetition\n",
    ")\n",
    "\n",
    "with torch.no_grad(): # Disable gradient calculation for inference\n",
    "    outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "\n",
    "# --- Decode and Display Results ---\n",
    "# Decode only the newly generated tokens (slice the output tensor)\n",
    "# Ensure slicing accounts for the prompt length correctly\n",
    "input_token_len = inputs['input_ids'].shape[1]\n",
    "generated_token_ids = outputs[0][input_token_len:]\n",
    "generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n--- Results ---\")\n",
    "print(f\"Raw Generated Text: '{generated_text}'\")\n",
    "\n",
    "# Attempt to parse the prediction\n",
    "predicted_answer = generated_text.strip().upper() # Remove whitespace, uppercase\n",
    "parsed_key = None\n",
    "if predicted_answer and predicted_answer[0] in ['A', 'B', 'C', 'D', 'E']:\n",
    "     parsed_key = predicted_answer[0]\n",
    "     print(f\"Parsed Predicted Key: {parsed_key}\")\n",
    "     if parsed_key == true_answer_key:\n",
    "         print(\"Outcome: CORRECT\")\n",
    "     else:\n",
    "         print(\"Outcome: INCORRECT\")\n",
    "else:\n",
    "     print(f\"Outcome: Could not parse a valid key (A-E) from generation: '{predicted_answer}'\")\n",
    "\n",
    "# (Optional) Clean up GPU memory if running multiple inferences\n",
    "# del model\n",
    "# del base_model\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation on Validation Set (After Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdanielbetschart\u001b[0m (\u001b[33mdanielbetschart-hochschule-luzern\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/DSPRO2/notebooks/wandb/run-20250418_181232-0ksgzg8k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/danielbetschart-hochschule-luzern/huggingface/runs/0ksgzg8k' target=\"_blank\">./results_llama2_7b_commonsenseqa</a></strong> to <a href='https://wandb.ai/danielbetschart-hochschule-luzern/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/huggingface' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/huggingface/runs/0ksgzg8k' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/huggingface/runs/0ksgzg8k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./results_llama2_7b_commonsenseqa/final_adapter)... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully resumed W&B run: 0ksgzg8k\n",
      "\n",
      "Logging adapter files from './results_llama2_7b_commonsenseqa/final_adapter' as a W&B Artifact...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done. 1.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact 'falcon-7b-commonsenseqa-adapter' logged successfully to run 0ksgzg8k.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>20867199140271360</td></tr><tr><td>train/epoch</td><td>0.99949</td></tr><tr><td>train/global_step</td><td>1217</td></tr><tr><td>train/grad_norm</td><td>0.53161</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.7234</td></tr><tr><td>train_loss</td><td>1.66493</td></tr><tr><td>train_runtime</td><td>8272.8599</td></tr><tr><td>train_samples_per_second</td><td>1.177</td></tr><tr><td>train_steps_per_second</td><td>0.147</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">./results_llama2_7b_commonsenseqa</strong> at: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/huggingface/runs/0ksgzg8k' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/huggingface/runs/0ksgzg8k</a><br> View project at: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/huggingface' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 7 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250418_181232-0ksgzg8k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B run finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm.notebook import tqdm # Progress bar\n",
    "import numpy as np\n",
    "import wandb\n",
    "import os # For path joining\n",
    "\n",
    "# --- Configuration (Ensure these match your trained model!) ---\n",
    "base_model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "# ===> IMPORTANT: UPDATE THIS PATH <===\n",
    "# Point this to the checkpoint or final saved adapter directory you want to evaluate\n",
    "adapter_model_dir = \"./results_llama2_7b_commonsenseqa/final_adapter\" # <--- REPLACE Appropriately\n",
    "\n",
    "if not os.path.isdir(adapter_model_dir):\n",
    "     print(f\"ERROR: Adapter directory not found: {adapter_model_dir}\")\n",
    "     # Add logic to stop or raise error\n",
    "     # raise FileNotFoundError(f\"Adapter directory not found: {adapter_model_dir}\")\n",
    "\n",
    "\n",
    "dataset_name = \"tau/commonsense_qa\"\n",
    "split_to_evaluate = \"validation\" # Or \"test\" if you want final test metrics\n",
    "# Optional: Initialize W&B if you want to log evaluation metrics to a specific run\n",
    "# EVAL_RUN_ID = \"abcdef12\" # ID of the run you want to associate these metrics with\n",
    "# wandb.init(project=\"falcon-commonsenseqa-sweep\", id=EVAL_RUN_ID, resume=\"allow\")\n",
    "\n",
    "\n",
    "# --- Reload Quantization Config (Must match training) ---\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# --- Load Base Model & Tokenizer ---\n",
    "print(f\"Loading base model: {base_model_name}\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    return_dict=True,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True if \"falcon\" in base_model_name else False,\n",
    "    token=None\n",
    ")\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_model_dir, token=None) # Load tokenizer saved with adapter\n",
    "if tokenizer.pad_token is None:\n",
    "     tokenizer.pad_token = tokenizer.eos_token\n",
    "     tokenizer.padding_side = \"right\" # Use right padding for generation\n",
    "\n",
    "# --- Load PEFT Adapter ---\n",
    "print(f\"Loading PEFT adapter from: {adapter_model_dir}\")\n",
    "model = PeftModel.from_pretrained(base_model, adapter_model_dir)\n",
    "model.eval() # Set to evaluation mode\n",
    "device = model.device\n",
    "print(\"Model ready for evaluation.\")\n",
    "\n",
    "# --- Load Dataset Split ---\n",
    "print(f\"Loading dataset split: {split_to_evaluate}\")\n",
    "eval_dataset = load_dataset(dataset_name, split=split_to_evaluate)\n",
    "\n",
    "# --- Prepare for Evaluation ---\n",
    "label_map = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
    "reverse_label_map = {v: k for k, v in label_map.items()} # For debugging\n",
    "y_true_eval = []\n",
    "y_pred_eval = []\n",
    "y_pred_parsed_keys = [] # Store parsed keys (A, B, C, D, E) or None\n",
    "y_true_keys = [] # Store true keys\n",
    "\n",
    "# Generation config (deterministic for evaluation)\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=5, # Enough for the letter + maybe EOS/newline\n",
    "    temperature=0.1,  # Low temp\n",
    "    do_sample=False,  # Greedy decoding\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# --- Evaluation Loop ---\n",
    "print(f\"Running evaluation on {len(eval_dataset)} samples from '{split_to_evaluate}' split...\")\n",
    "num_parse_errors = 0\n",
    "for i in tqdm(range(len(eval_dataset))):\n",
    "    sample = eval_dataset[i]\n",
    "    question = sample['question']\n",
    "    choices_text = sample['choices']['text']\n",
    "    choices_labels = sample['choices']['label']\n",
    "    true_answer_key = sample['answerKey']\n",
    "    true_label_numeric = label_map.get(true_answer_key, -1) # Handle potential invalid keys in data\n",
    "\n",
    "    y_true_keys.append(true_answer_key)\n",
    "    y_true_eval.append(true_label_numeric)\n",
    "\n",
    "    # Format prompt\n",
    "    inference_prompt = f\"### Question:\\n{question}\\n\\n### Choices:\\n\"\n",
    "    for label, text in zip(choices_labels, choices_text):\n",
    "        inference_prompt += f\"{label}) {text}\\n\"\n",
    "    inference_prompt += f\"\\n### Answer:\\n\"\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(inference_prompt, return_tensors=\"pt\", padding=False).to(device)\n",
    "\n",
    "    # Generate\n",
    "    predicted_key_numeric = -1 # Default to -1 for parse failure\n",
    "    predicted_parsed_key = None\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "        # Decode generated part\n",
    "        input_token_len = inputs['input_ids'].shape[1]\n",
    "        generated_token_ids = outputs[0][input_token_len:]\n",
    "        generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Parse prediction\n",
    "        parsed_pred = generated_text.strip().upper()\n",
    "        if parsed_pred and parsed_pred[0] in label_map:\n",
    "            predicted_parsed_key = parsed_pred[0]\n",
    "            predicted_key_numeric = label_map[predicted_parsed_key]\n",
    "        else:\n",
    "            num_parse_errors += 1\n",
    "            # Optional: Log the failed parse\n",
    "            # print(f\"Parse Error (Index {i}): Prompt:\\n{inference_prompt}\\nGenerated: '{generated_text}' Parsed: '{parsed_pred}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during generation/parsing for index {i}: {e}\")\n",
    "        num_parse_errors += 1\n",
    "        # Keep predicted_key_numeric as -1 and predicted_parsed_key as None\n",
    "\n",
    "    y_pred_eval.append(predicted_key_numeric)\n",
    "    y_pred_parsed_keys.append(predicted_parsed_key)\n",
    "\n",
    "\n",
    "print(\"Evaluation loop finished.\")\n",
    "print(f\"Total parse errors: {num_parse_errors}\")\n",
    "\n",
    "# --- Calculate Metrics ---\n",
    "# Overall accuracy includes parse failures (counted as incorrect)\n",
    "correct_predictions = sum(1 for true, pred in zip(y_true_eval, y_pred_eval) if true == pred and true != -1)\n",
    "total_samples = len(y_true_eval)\n",
    "accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "# For Precision, Recall, F1, only consider samples where parsing succeeded *and* true label was valid\n",
    "valid_indices = [i for i, (p, t) in enumerate(zip(y_pred_eval, y_true_eval)) if p != -1 and t != -1]\n",
    "\n",
    "if len(valid_indices) > 0:\n",
    "    filtered_y_true = [y_true_eval[i] for i in valid_indices]\n",
    "    filtered_y_pred = [y_pred_eval[i] for i in valid_indices]\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        filtered_y_true,\n",
    "        filtered_y_pred,\n",
    "        average='macro', # Average metrics across classes (A, B, C, D, E)\n",
    "        zero_division=0   # Set metric to 0 if no predictions/true labels for a class\n",
    "    )\n",
    "    num_parsed = len(valid_indices)\n",
    "else:\n",
    "    print(\"Warning: Could not parse any valid predictions (A-E), or no valid true labels found. Precision/Recall/F1 will be 0.\")\n",
    "    precision, recall, f1 = 0.0, 0.0, 0.0\n",
    "    num_parsed = 0\n",
    "\n",
    "\n",
    "# --- Print Results ---\n",
    "print(\"\\n--- Evaluation Metrics ---\")\n",
    "print(f\"Split Evaluated:        {split_to_evaluate}\")\n",
    "print(f\"Total Samples:          {total_samples}\")\n",
    "print(f\"Successfully Parsed:    {num_parsed} ({num_parsed/total_samples:.1%} of total, excluding invalid true labels)\")\n",
    "print(f\"Accuracy (overall):     {accuracy:.4f} (Correct predictions / Total samples)\")\n",
    "print(f\"Precision (macro, parsed only): {precision:.4f}\")\n",
    "print(f\"Recall (macro, parsed only):    {recall:.4f}\")\n",
    "print(f\"F1 Score (macro, parsed only):  {f1:.4f}\")\n",
    "\n",
    "# --- Log to W&B if initialized ---\n",
    "if wandb.run:\n",
    "    wandb.log({\n",
    "        f\"{split_to_evaluate}_accuracy\": accuracy,\n",
    "        f\"{split_to_evaluate}_precision_macro\": precision,\n",
    "        f\"{split_to_evaluate}_recall_macro\": recall,\n",
    "        f\"{split_to_evaluate}_f1_macro\": f1,\n",
    "        f\"{split_to_evaluate}_parsed_count\": num_parsed,\n",
    "        f\"{split_to_evaluate}_total_count\": total_samples,\n",
    "        f\"{split_to_evaluate}_parse_errors\": num_parse_errors\n",
    "    })\n",
    "    print(f\"Metrics logged to W&B run: {wandb.run.path}\")\n",
    "    # Optional: Finish the run if it was only for evaluation\n",
    "    # wandb.finish()\n",
    "else:\n",
    "    print(\"W&B run not active. Metrics not logged.\")\n",
    "\n",
    "\n",
    "# (Optional) Clean up GPU memory\n",
    "del model\n",
    "del base_model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"CUDA cache cleared after evaluation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
