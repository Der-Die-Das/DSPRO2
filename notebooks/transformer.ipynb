{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 datasets evaluate sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 9741\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 1221\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 1140\n",
      "    })\n",
      "})\n",
      "\n",
      "Example Train instance:\n",
      "{'id': '075e483d21c29a511267ef62bedc0461', 'question': 'The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?', 'question_concept': 'punishing', 'choices': {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['ignore', 'enforce', 'authoritarian', 'yell at', 'avoid']}, 'answerKey': 'A'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"tau/commonsense_qa\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "print(\"Dataset loaded:\")\n",
    "print(dataset)\n",
    "print(\"\\nExample Train instance:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration & Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, # Using CausalLM because Llama is generative\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "\n",
    "# --- Model Configuration ---\n",
    "model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "# --- QLoRA Configuration ---\n",
    "use_4bit = True             # Activate 4-bit precision base model loading\n",
    "bnb_4bit_compute_dtype = \"float16\" # Compute dtype for 4-bit base models\n",
    "bnb_4bit_quant_type = \"nf4\" # Quantization type (fp4 or nf4)\n",
    "use_nested_quant = False    # Activate nested quantization for 4-bit base models (double quantization)\n",
    "\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# --- LoRA Configuration ---\n",
    "lora_r = 64                 # LoRA attention dimension\n",
    "lora_alpha = 16             # Alpha parameter for LoRA scaling\n",
    "lora_dropout = 0.1          # Dropout probability for LoRA layers\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # --- Update target modules for Falcon 7B ---\n",
    "    target_modules=[\n",
    "        \"query_key_value\",\n",
    "        \"dense\",\n",
    "        \"dense_h_to_4h\",\n",
    "        \"dense_4h_to_h\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# --- Training Arguments Configuration ---\n",
    "output_dir = \"./results_llama2_7b_commonsenseqa\" # Directory to save results/checkpoints\n",
    "num_train_epochs = 1        # Start with 1 epoch for initial testing\n",
    "fp16 = False                # Enable fp16 training (set bf16=True if supported)\n",
    "bf16 = False                # Enable bf16 training (requires Ampere GPU or newer)\n",
    "per_device_train_batch_size = 1 # VERY IMPORTANT: Start low due to memory constraints\n",
    "per_device_eval_batch_size = 1  # VERY IMPORTANT: Start low due to memory constraints\n",
    "gradient_accumulation_steps = 8 # Simulate larger batch size (effective batch size = train_batch_size * accumulation_steps)\n",
    "gradient_checkpointing = True   # Enable gradient checkpointing to save memory\n",
    "max_grad_norm = 0.3         # Max gradient norm for clipping\n",
    "learning_rate = 2e-4        # Initial learning rate (AdamW optimizer)\n",
    "weight_decay = 0.001        # Weight decay for AdamW if we apply it\n",
    "optim = \"paged_adamw_32bit\" # Use paged optimizer to save memory\n",
    "lr_scheduler_type = \"cosine\" # Learning rate schedule\n",
    "max_steps = -1              # Number of training steps (overrides num_train_epochs if > 0)\n",
    "warmup_ratio = 0.03         # Ratio of steps for linear warmup (from 0 to learning rate)\n",
    "group_by_length = True      # Group sequences into batches with similar lengths (saves memory & speeds up training)\n",
    "save_steps = 50             # Save checkpoint every X updates steps (adjust as needed)\n",
    "logging_steps = 10          # Log metrics every X updates steps (adjust as needed)\n",
    "\n",
    "# --- SFTTrianer Specific (using standard Trainer for now, but TRL's SFT is often used) ---\n",
    "# max_seq_length = None # Maximum sequence length to use (can be helpful)\n",
    "# packing = False # Pack multiple short examples in the same input sequence to increase efficiency\n",
    "\n",
    "device_map = {\"\": 0} # Load the entire model on the default GPU (GPU 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d75f02baac14d00b05fc6f126587fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer loaded.\n",
      "Model Configuration: FalconConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"alibi\": false,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"FalconForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"tiiuae/falcon-7b-instruct--configuration_falcon.FalconConfig\",\n",
      "    \"AutoModel\": \"tiiuae/falcon-7b-instruct--modeling_falcon.FalconModel\",\n",
      "    \"AutoModelForCausalLM\": \"tiiuae/falcon-7b-instruct--modeling_falcon.FalconForCausalLM\",\n",
      "    \"AutoModelForQuestionAnswering\": \"tiiuae/falcon-7b-instruct--modeling_falcon.FalconForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"tiiuae/falcon-7b-instruct--modeling_falcon.FalconForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"tiiuae/falcon-7b-instruct--modeling_falcon.FalconForTokenClassification\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 11,\n",
      "  \"eos_token_id\": 11,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4544,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"falcon\",\n",
      "  \"multi_query\": true,\n",
      "  \"new_decoder_architecture\": false,\n",
      "  \"num_attention_heads\": 71,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_kv_heads\": 71,\n",
      "  \"parallel_attn\": true,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": false,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "trainable params: 130,547,712 || all params: 7,052,268,416 || trainable%: 1.8511\n",
      "\n",
      "PEFT Model ready.\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    token=None,\n",
    "    trust_remote_code=True # <-- ADD FOR FALCON\n",
    ")\n",
    "model.config.use_cache = False # Necessary for gradient checkpointing\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    # trust_remote_code=True, # Usually not needed for tokenizer, but add if issues persist\n",
    "    token=None\n",
    ")\n",
    "# --- IMPORTANT: Set Padding Token ---\n",
    "# Llama usually doesn't have a pad token by default. Use EOS token as pad token.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fine-tuning generative models works best with right-padding\n",
    "\n",
    "print(\"Model and Tokenizer loaded.\")\n",
    "print(\"Model Configuration:\", model.config)\n",
    "\n",
    "# --- Prepare model for QLoRA ---\n",
    "# model = prepare_model_for_kbit_training(model) # Handled by PEFT library >= 0.4.0\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nPEFT Model ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing - Format Data as Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example Formatted Prompt (for training):\n",
      "### Question:\n",
      "The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
      "\n",
      "### Choices:\n",
      "A) ignore\n",
      "B) enforce\n",
      "C) authoritarian\n",
      "D) yell at\n",
      "E) avoid\n",
      "\n",
      "### Answer:\n",
      "A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38f0ab9a7bf4b20887b29d58a4ba242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9741 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 9741\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 1221\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 1140\n",
      "    })\n",
      "})\n",
      "\n",
      "Example tokenized input_ids:\n"
     ]
    }
   ],
   "source": [
    "# --- How Llama needs to see the data ---\n",
    "# We'll format each example as a prompt where the model's task is to predict the correct answer letter.\n",
    "# Example Format:\n",
    "# ### Question:\n",
    "# [Question Text]\n",
    "# ### Choices:\n",
    "# A) [Choice A Text]\n",
    "# B) [Choice B Text]\n",
    "# C) [Choice C Text]\n",
    "# D) [Choice D Text]\n",
    "# E) [Choice E Text]\n",
    "# ### Answer:\n",
    "# [Correct Answer Letter (A, B, C, D, or E)] <--- This is what the model should generate\n",
    "\n",
    "def format_prompt(example):\n",
    "    question = example['question']\n",
    "    choices_text = example['choices']['text']\n",
    "    choices_labels = example['choices']['label'] # Should be ['A', 'B', 'C', 'D', 'E']\n",
    "    answer_key = example['answerKey'] # The correct label ('A', 'B', 'C', 'D', or 'E')\n",
    "\n",
    "    prompt = f\"### Question:\\n{question}\\n\\n### Choices:\\n\"\n",
    "    for label, text in zip(choices_labels, choices_text):\n",
    "        prompt += f\"{label}) {text}\\n\"\n",
    "\n",
    "    prompt += f\"\\n### Answer:\\n{answer_key}\" # Include the answer for training\n",
    "    return {\"text\": prompt} # We are creating a single text field for the trainer\n",
    "\n",
    "# Apply formatting (this might take a moment)\n",
    "# Note: This creates prompts INCLUDING the answer for fine-tuning.\n",
    "formatted_dataset = dataset.map(format_prompt, remove_columns=list(dataset['train'].features))\n",
    "\n",
    "print(\"\\nExample Formatted Prompt (for training):\")\n",
    "print(formatted_dataset['train'][0]['text'])\n",
    "\n",
    "# --- Tokenize the formatted text ---\n",
    "# We need to tokenize the 'text' field created above.\n",
    "# Let's set a reasonable max_length. Analyze dataset if needed, start with 256 or 512.\n",
    "max_sequence_length = 256 # Adjust based on typical prompt length and GPU memory\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False, # Let the trainer/collator handle padding if needed, or manage here\n",
    "        max_length=max_sequence_length,\n",
    "        # return_overflowing_tokens=True, # Be careful with this\n",
    "        # return_length=True,\n",
    "    )\n",
    "\n",
    "# Tokenize the dataset\n",
    "# remove_columns needed because map adds the tokenization outputs but doesn't auto-remove original text\n",
    "tokenized_dataset = formatted_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "print(\"\\nTokenized dataset structure:\")\n",
    "print(tokenized_dataset)\n",
    "print(\"\\nExample tokenized input_ids:\")\n",
    "# print(tokenized_dataset['train'][0]['input_ids']) # Might be long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer # SFTTrainer is often easier for generative fine-tuning\n",
    "\n",
    "# --- Alternative: Using standard Trainer (more setup required) ---\n",
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"wandb\", # or \"wandb\" if you have it configured\n",
    "    # --- Evaluation Args (Need custom compute_metrics for generation) ---\n",
    "    # evaluation_strategy=\"steps\", # Evaluate periodically\n",
    "    # eval_steps=50,               # How often to evaluate\n",
    "    # per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    # load_best_model_at_end=True, # Usually good practice\n",
    "    # metric_for_best_model=\"eval_loss\", # Or a custom metric if defined\n",
    ")\n",
    "\n",
    "# --- Using SFTTrainer from TRL (Simpler for prompt tuning) ---\n",
    "'''\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    # eval_dataset=tokenized_dataset[\"validation\"], # Needs careful handling for generation eval\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\", # Need to re-map formatted_dataset if using SFTTrainer directly\n",
    "                               # Or use a custom data collator with standard Trainer\n",
    "                               # Sticking with standard Trainer approach for now based on prior code\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    max_seq_length=max_sequence_length, # Pass max_seq_length to SFTTrainer\n",
    "    packing=False, # Set packing based on config\n",
    "    # --- Need to adjust if using SFTTrainer ---\n",
    "    # For SFTTrainer, the input dataset should ideally just have the 'text' field\n",
    "    # Let's revert to standard Trainer and handle data collation manually if needed.\n",
    ")\n",
    "'''\n",
    "# --- Revert to standard Trainer ---\n",
    "# Need a data collator that handles causal LM masking properly\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    # tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics, # Still commented out\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/DSPRO2/notebooks/wandb/run-20250418_134544-0ksgzg8k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danielbetschart-hochschule-luzern/huggingface/runs/0ksgzg8k' target=\"_blank\">./results_llama2_7b_commonsenseqa</a></strong> to <a href='https://wandb.ai/danielbetschart-hochschule-luzern/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/huggingface' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/huggingface/runs/0ksgzg8k' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/huggingface/runs/0ksgzg8k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1217' max='1217' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1217/1217 2:17:43, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.939700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.547500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.728600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.557300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.070700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.972200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.780400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.681600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.611500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.099900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.932700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.789900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.592900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.485200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.057100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.840200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.701700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.591100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.448100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.978100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.828600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.692100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.630900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.491900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.959700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.781700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.689800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.595100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.444500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.854300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.761900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.667100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.582600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.490600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.941000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.751100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.608800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.516500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.465900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.863000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.757100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.653200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.443200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.885000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.745100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.698900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.526600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.437400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.909600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.739300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.673900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.601900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.435000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.916800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>1.754200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.646100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>1.554200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.496900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>1.814100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.685200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.604700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.515900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.387200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.859100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>1.700300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.650500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>1.491400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.396600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>1.868200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.716000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>1.644400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.462400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.431800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.806600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>1.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.646400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>1.520300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>1.791100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.669700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>1.564100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.536900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.387300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>1.819700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>1.715000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.575400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>1.462300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.336500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>1.844100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.628000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>1.583400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>1.446400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.394600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.771600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>1.673000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1.537400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>1.458900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.304200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>1.840200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>1.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>1.558600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>1.431000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.407600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>1.781100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>1.658100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>1.504700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>1.798400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>1.701900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>1.634600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>1.522800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.377200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>1.839700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>1.672500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>1.531000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>1.427900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.410300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>1.723400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "***** train metrics *****\n",
      "  epoch                    =     0.9995\n",
      "  total_flos               = 19434093GF\n",
      "  train_loss               =     1.6649\n",
      "  train_runtime            = 2:17:52.85\n",
      "  train_samples_per_second =      1.177\n",
      "  train_steps_per_second   =      0.147\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "# This will take a significant amount of time and requires a capable GPU.\n",
    "# Monitor the loss in the output logs.\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# --- Save training metrics ---\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Final Model (Adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the final PEFT adapter model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving the final PEFT adapter model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39msave_state() \u001b[38;5;66;03m# Save trainer state\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# The PEFT adapter weights are saved in the output_dir checkpoints\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# To save the final adapter separately:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m final_adapter_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/final_adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Saving the final PEFT adapter model...\")\n",
    "trainer.save_state() # Save trainer state\n",
    "# The PEFT adapter weights are saved in the output_dir checkpoints\n",
    "# To save the final adapter separately:\n",
    "final_adapter_dir = f\"{output_dir}/final_adapter\"\n",
    "model.save_pretrained(final_adapter_dir)\n",
    "tokenizer.save_pretrained(final_adapter_dir)\n",
    "print(f\"Final PEFT adapter saved to {final_adapter_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Upload of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdanielbetschart\u001b[0m (\u001b[33mdanielbetschart-hochschule-luzern\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/DSPRO2/notebooks/wandb/run-20250418_181232-0ksgzg8k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/danielbetschart-hochschule-luzern/huggingface/runs/0ksgzg8k' target=\"_blank\">./results_llama2_7b_commonsenseqa</a></strong> to <a href='https://wandb.ai/danielbetschart-hochschule-luzern/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/huggingface' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielbetschart-hochschule-luzern/huggingface/runs/0ksgzg8k' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/huggingface/runs/0ksgzg8k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./results_llama2_7b_commonsenseqa/final_adapter)... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully resumed W&B run: 0ksgzg8k\n",
      "\n",
      "Logging adapter files from './results_llama2_7b_commonsenseqa/final_adapter' as a W&B Artifact...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done. 1.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact 'falcon-7b-commonsenseqa-adapter' logged successfully to run 0ksgzg8k.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>20867199140271360</td></tr><tr><td>train/epoch</td><td>0.99949</td></tr><tr><td>train/global_step</td><td>1217</td></tr><tr><td>train/grad_norm</td><td>0.53161</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.7234</td></tr><tr><td>train_loss</td><td>1.66493</td></tr><tr><td>train_runtime</td><td>8272.8599</td></tr><tr><td>train_samples_per_second</td><td>1.177</td></tr><tr><td>train_steps_per_second</td><td>0.147</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">./results_llama2_7b_commonsenseqa</strong> at: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/huggingface/runs/0ksgzg8k' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/huggingface/runs/0ksgzg8k</a><br> View project at: <a href='https://wandb.ai/danielbetschart-hochschule-luzern/huggingface' target=\"_blank\">https://wandb.ai/danielbetschart-hochschule-luzern/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 7 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250418_181232-0ksgzg8k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B run finished.\n"
     ]
    }
   ],
   "source": [
    "# --- Run this in a NEW cell or script AFTER training finished ---\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "YOUR_RUN_ID = \"0ksgzg8k\" # <--- REPLACE THIS with the actual ID of your finished training run\n",
    "YOUR_PROJECT_NAME = \"huggingface\" # <--- REPLACE THIS with your W&B project name (check dashboard)\n",
    "YOUR_ENTITY_NAME = \"danielbetschart-hochschule-luzern\" # <--- REPLACE THIS with your W&B username/entity\n",
    "\n",
    "# Directory where your final adapter and tokenizer were saved\n",
    "adapter_output_dir = \"./results_llama2_7b_commonsenseqa/final_adapter\" #<--- MAKE SURE THIS IS CORRECT\n",
    "# A descriptive name for the artifact in W&B\n",
    "artifact_name = \"falcon-7b-commonsenseqa-adapter\" # You can keep this simple now\n",
    "# Base model name for metadata\n",
    "base_model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "# Training details for metadata (get these from your config/args if possible)\n",
    "use_4bit = True # Example\n",
    "num_train_epochs = 1 # Example\n",
    "\n",
    "# --- Login if in a new session ---\n",
    "wandb.login() # Uncomment and run if needed\n",
    "\n",
    "# --- Resume the specific run ---\n",
    "try:\n",
    "    resumed_run = wandb.init(\n",
    "        project=YOUR_PROJECT_NAME,\n",
    "        entity=YOUR_ENTITY_NAME,\n",
    "        id=YOUR_RUN_ID,\n",
    "        resume=\"must\" # Essential: tells wandb to reconnect to the existing run\n",
    "    )\n",
    "    print(f\"Successfully resumed W&B run: {resumed_run.id}\")\n",
    "\n",
    "    # --- Create and Log W&B Artifact ---\n",
    "    print(f\"\\nLogging adapter files from '{adapter_output_dir}' as a W&B Artifact...\")\n",
    "    try:\n",
    "        # Create an artifact object\n",
    "        adapter_artifact = wandb.Artifact(\n",
    "            name=artifact_name,\n",
    "            type=\"model\",\n",
    "            description=f\"PEFT LoRA adapter for {base_model_name} fine-tuned on CommonsenseQA (Manually uploaded).\",\n",
    "            metadata={\"base_model\": base_model_name,\n",
    "                      \"finetuning_task\": \"commonsenseqa_prompt_completion\",\n",
    "                      \"quantization\": \"4-bit NF4\" if use_4bit else \"None\",\n",
    "                      \"epochs\": num_train_epochs}\n",
    "        )\n",
    "\n",
    "        # Add the entire directory containing the adapter files\n",
    "        adapter_artifact.add_dir(adapter_output_dir)\n",
    "\n",
    "        # Log the artifact to the RESUMED W&B run\n",
    "        resumed_run.log_artifact(adapter_artifact) # Use the run object returned by init\n",
    "\n",
    "        print(f\"Artifact '{artifact_name}' logged successfully to run {resumed_run.id}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error logging W&B artifact: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # --- Finish the resumed run ---\n",
    "        resumed_run.finish()\n",
    "        print(\"W&B run finished.\")\n",
    "\n",
    "except wandb.errors.UsageError as e:\n",
    "    print(f\"Error resuming W&B run (maybe ID is wrong or run never existed?): {e}\")\n",
    "except Exception as e:\n",
    "     print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: tiiuae/falcon-7b-instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a65766bef14cb894ccfad51e34d97f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded.\n",
      "Loading tokenizer from adapter directory: ./results_llama2_7b_commonsenseqa/final_adapter\n",
      "Tokenizer loaded.\n",
      "Loading PEFT adapter from: ./results_llama2_7b_commonsenseqa/final_adapter\n",
      "PEFT adapter loaded.\n",
      "Model set to evaluation mode.\n",
      "Loading original CommonsenseQA dataset...\n",
      "Dataset loaded.\n",
      "\n",
      "--- Running Inference ---\n",
      "Sample Index: 1129\n",
      "\n",
      "Input Prompt Sent to Model:\n",
      "---------------------------\n",
      "### Question:\n",
      "Where is a good place to have a fireplace in a house?\n",
      "\n",
      "### Choices:\n",
      "A) big house\n",
      "B) train\n",
      "C) cabin\n",
      "D) living room\n",
      "E) home\n",
      "\n",
      "### Answer:\n",
      "\n",
      "---------------------------\n",
      "Actual Answer Key: D\n",
      "\n",
      "Generating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results ---\n",
      "Raw Generated Text: 'D) home\n",
      "\n",
      "###'\n",
      "Parsed Predicted Key: D\n",
      "Outcome: CORRECT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig # Added for generation parameters\n",
    ")\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Use the SAME base model name you trained with (Falcon or Mistral)\n",
    "base_model_name = \"tiiuae/falcon-7b-instruct\" # Or \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# IMPORTANT: Set this to the directory where your TRAINED ADAPTER was saved\n",
    "# It might be './results_falcon_7b_commonsenseqa/final_adapter'\n",
    "# Or it might be a specific checkpoint like './results_falcon_7b_commonsenseqa/checkpoint-1000'\n",
    "adapter_model_dir = \"./results_llama2_7b_commonsenseqa/final_adapter\" # <--- UPDATE THIS PATH\n",
    "\n",
    "# --- Reload Quantization Config (Must match training) ---\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "device_map = {\"\": 0} # Load model on default GPU\n",
    "\n",
    "# --- Load Base Model ---\n",
    "print(f\"Loading base model: {base_model_name}\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    # low_cpu_mem_usage=True, # Can help if CPU RAM is limited\n",
    "    return_dict=True,\n",
    "    torch_dtype=compute_dtype, # Use compute_dtype\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True if \"falcon\" in base_model_name else False, # Needed for Falcon\n",
    "    token=None # Ensure no token is used for open models\n",
    ")\n",
    "print(\"Base model loaded.\")\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "# Load the tokenizer saved WITH THE ADAPTER (ensures consistency)\n",
    "print(f\"Loading tokenizer from adapter directory: {adapter_model_dir}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_model_dir, token=None)\n",
    "# Ensure padding token is set correctly (usually EOS for these models)\n",
    "if tokenizer.pad_token is None:\n",
    "     tokenizer.pad_token = tokenizer.eos_token\n",
    "     tokenizer.padding_side = \"right\" # Important for generation\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "# --- Load PEFT Adapter ---\n",
    "print(f\"Loading PEFT adapter from: {adapter_model_dir}\")\n",
    "# Load the LoRA adapter onto the base model\n",
    "model = PeftModel.from_pretrained(base_model, adapter_model_dir)\n",
    "print(\"PEFT adapter loaded.\")\n",
    "\n",
    "# --- Merge Adapter (Optional, for faster inference but uses more RAM initially) ---\n",
    "# print(\"Merging adapter weights...\")\n",
    "# model = model.merge_and_unload()\n",
    "# print(\"Adapter merged.\")\n",
    "\n",
    "# --- Set to Evaluation Mode ---\n",
    "model.eval()\n",
    "print(\"Model set to evaluation mode.\")\n",
    "\n",
    "# --- Load Original Dataset (for picking a sample) ---\n",
    "print(\"Loading original CommonsenseQA dataset...\")\n",
    "original_dataset = load_dataset(\"tau/commonsense_qa\")\n",
    "validation_data = original_dataset['validation']\n",
    "print(\"Dataset loaded.\")\n",
    "\n",
    "# --- Select a Random Validation Sample ---\n",
    "random_index = random.randint(0, len(validation_data) - 1)\n",
    "sample = validation_data[random_index]\n",
    "\n",
    "# --- Prepare Prompt for Inference (WITHOUT the answer) ---\n",
    "question = sample['question']\n",
    "choices_text = sample['choices']['text']\n",
    "choices_labels = sample['choices']['label']\n",
    "true_answer_key = sample['answerKey']\n",
    "\n",
    "# Format prompt exactly as used in training, but stop before the answer\n",
    "inference_prompt = f\"### Question:\\n{question}\\n\\n### Choices:\\n\"\n",
    "for label, text in zip(choices_labels, choices_text):\n",
    "    inference_prompt += f\"{label}) {text}\\n\"\n",
    "inference_prompt += f\"\\n### Answer:\\n\" # Model generates what comes next\n",
    "\n",
    "# --- Tokenize the Inference Prompt ---\n",
    "device = model.device # Get the device the model is on\n",
    "inputs = tokenizer(inference_prompt, return_tensors=\"pt\", padding=False).to(device)\n",
    "\n",
    "# --- Generate the Answer ---\n",
    "print(\"\\n--- Running Inference ---\")\n",
    "print(f\"Sample Index: {random_index}\")\n",
    "print(\"\\nInput Prompt Sent to Model:\")\n",
    "print(\"---------------------------\")\n",
    "print(inference_prompt)\n",
    "print(\"---------------------------\")\n",
    "print(f\"Actual Answer Key: {true_answer_key}\")\n",
    "print(\"\\nGenerating...\")\n",
    "\n",
    "# Configuration for generation\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=5,       # Generate only a few tokens (A, B, C, D, E + maybe newline/EOS)\n",
    "    temperature=0.1,        # Low temperature for deterministic output\n",
    "    top_p=0.9,              # Can adjust, but low temp is often enough\n",
    "    do_sample=False,        # Use greedy decoding (most likely token)\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "with torch.no_grad(): # Disable gradient calculation for inference\n",
    "    outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "\n",
    "# --- Decode and Display Results ---\n",
    "# Decode only the newly generated tokens (slice the output tensor)\n",
    "generated_token_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n--- Results ---\")\n",
    "print(f\"Raw Generated Text: '{generated_text}'\")\n",
    "\n",
    "# Attempt to parse the prediction\n",
    "predicted_answer = generated_text.strip().upper() # Remove whitespace, uppercase\n",
    "parsed_key = None\n",
    "if predicted_answer and predicted_answer[0] in ['A', 'B', 'C', 'D', 'E']:\n",
    "     parsed_key = predicted_answer[0]\n",
    "     print(f\"Parsed Predicted Key: {parsed_key}\")\n",
    "     if parsed_key == true_answer_key:\n",
    "         print(\"Outcome: CORRECT\")\n",
    "     else:\n",
    "         print(\"Outcome: INCORRECT\")\n",
    "else:\n",
    "     print(\"Outcome: Could not parse a valid key (A-E) from generation.\")\n",
    "\n",
    "# (Optional) Clean up GPU memory if needed\n",
    "# del model\n",
    "# del base_model\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: tiiuae/falcon-7b-instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ba975ed2ca4656ba4b6da4dd2c572f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading PEFT adapter from: ./results_llama2_7b_commonsenseqa/final_adapter\n",
      "Model ready for evaluation.\n",
      "Loading dataset split: validation\n",
      "Running evaluation on 1221 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b691ca9dd6e04cdd9d1d5653e1af0472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1221 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loop finished.\n",
      "\n",
      "--- Evaluation Metrics ---\n",
      "Split Evaluated:        validation\n",
      "Total Samples:          1221\n",
      "Successfully Parsed:    1221 (100.0% of total)\n",
      "Accuracy (overall):     0.5905\n",
      "Precision (macro, parsed only): 0.5900\n",
      "Recall (macro, parsed only):    0.5900\n",
      "F1 Score (macro, parsed only):  0.5898\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "You must call wandb.init() before wandb.log()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 156\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1 Score (macro, parsed only):  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Log to W&B if desired ---\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msplit_to_evaluate\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_accuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msplit_to_evaluate\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_precision_macro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msplit_to_evaluate\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_recall_macro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecall\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msplit_to_evaluate\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_f1_macro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msplit_to_evaluate\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_parsed_count\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_parsed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msplit_to_evaluate\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_total_count\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_samples\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# (Optional) Clean up GPU memory\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/wandb/sdk/lib/preinit.py:36\u001b[0m, in \u001b[0;36mPreInitCallable.<locals>.preinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreinit_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must call wandb.init() before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm.notebook import tqdm # Progress bar\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration (Ensure these match your trained model!) ---\n",
    "base_model_name = \"tiiuae/falcon-7b-instruct\" # Or \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "adapter_model_dir = \"./results_llama2_7b_commonsenseqa/final_adapter\" # <--- UPDATE THIS PATH\n",
    "dataset_name = \"tau/commonsense_qa\"\n",
    "split_to_evaluate = \"validation\" # Or \"test\" if you want final test metrics\n",
    "\n",
    "# --- Reload Quantization Config (Must match training) ---\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# --- Load Base Model & Tokenizer (Only if not already loaded and kernel restarted) ---\n",
    "# It's safer to reload to ensure a clean state for evaluation\n",
    "print(f\"Loading base model: {base_model_name}\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    return_dict=True,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True if \"falcon\" in base_model_name else False,\n",
    "    token=None\n",
    ")\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_model_dir, token=None) # Load tokenizer saved with adapter\n",
    "if tokenizer.pad_token is None:\n",
    "     tokenizer.pad_token = tokenizer.eos_token\n",
    "     tokenizer.padding_side = \"right\" # Use right padding for generation\n",
    "\n",
    "# --- Load PEFT Adapter ---\n",
    "print(f\"Loading PEFT adapter from: {adapter_model_dir}\")\n",
    "model = PeftModel.from_pretrained(base_model, adapter_model_dir)\n",
    "model.eval() # Set to evaluation mode\n",
    "device = model.device\n",
    "print(\"Model ready for evaluation.\")\n",
    "\n",
    "# --- Load Dataset Split ---\n",
    "print(f\"Loading dataset split: {split_to_evaluate}\")\n",
    "eval_dataset = load_dataset(dataset_name, split=split_to_evaluate)\n",
    "\n",
    "# --- Prepare for Evaluation ---\n",
    "label_map = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
    "y_true_eval = []\n",
    "y_pred_eval = []\n",
    "\n",
    "# Generation config (deterministic for evaluation)\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=5, # Enough for the letter + maybe EOS/newline\n",
    "    temperature=0.1,  # Low temp\n",
    "    do_sample=False,  # Greedy decoding\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# --- Evaluation Loop ---\n",
    "print(f\"Running evaluation on {len(eval_dataset)} samples...\")\n",
    "for i in tqdm(range(len(eval_dataset))):\n",
    "    sample = eval_dataset[i]\n",
    "    question = sample['question']\n",
    "    choices_text = sample['choices']['text']\n",
    "    choices_labels = sample['choices']['label']\n",
    "    true_answer_key = sample['answerKey']\n",
    "    true_label_numeric = label_map[true_answer_key]\n",
    "\n",
    "    # Format prompt\n",
    "    inference_prompt = f\"### Question:\\n{question}\\n\\n### Choices:\\n\"\n",
    "    for label, text in zip(choices_labels, choices_text):\n",
    "        inference_prompt += f\"{label}) {text}\\n\"\n",
    "    inference_prompt += f\"\\n### Answer:\\n\"\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(inference_prompt, return_tensors=\"pt\", padding=False).to(device)\n",
    "\n",
    "    # Generate\n",
    "    predicted_key_numeric = -1 # Default to -1 for parse failure\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "        # Decode generated part\n",
    "        generated_token_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Parse prediction\n",
    "        parsed_pred = generated_text.strip().upper()\n",
    "        if parsed_pred and parsed_pred[0] in label_map:\n",
    "            predicted_key_numeric = label_map[parsed_pred[0]]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during generation/parsing for index {i}: {e}\")\n",
    "        # Keep predicted_key_numeric as -1\n",
    "\n",
    "    y_true_eval.append(true_label_numeric)\n",
    "    y_pred_eval.append(predicted_key_numeric)\n",
    "\n",
    "print(\"Evaluation loop finished.\")\n",
    "\n",
    "# --- Calculate Metrics ---\n",
    "# Count parse failures as incorrect predictions for overall accuracy\n",
    "correct_predictions = sum(1 for true, pred in zip(y_true_eval, y_pred_eval) if true == pred)\n",
    "total_samples = len(y_true_eval)\n",
    "accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "# For Precision, Recall, F1, only consider samples where parsing succeeded\n",
    "valid_indices = [i for i, p in enumerate(y_pred_eval) if p != -1]\n",
    "if len(valid_indices) > 0:\n",
    "    filtered_y_true = [y_true_eval[i] for i in valid_indices]\n",
    "    filtered_y_pred = [y_pred_eval[i] for i in valid_indices]\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        filtered_y_true,\n",
    "        filtered_y_pred,\n",
    "        average='macro', # Average metrics across classes\n",
    "        zero_division=0   # Set metric to 0 if no predictions for a class\n",
    "    )\n",
    "    num_parsed = len(valid_indices)\n",
    "else:\n",
    "    print(\"Warning: Could not parse any valid predictions (A-E). Precision/Recall/F1 will be 0.\")\n",
    "    precision, recall, f1 = 0.0, 0.0, 0.0\n",
    "    num_parsed = 0\n",
    "\n",
    "\n",
    "# --- Print Results ---\n",
    "print(\"\\n--- Evaluation Metrics ---\")\n",
    "print(f\"Split Evaluated:        {split_to_evaluate}\")\n",
    "print(f\"Total Samples:          {total_samples}\")\n",
    "print(f\"Successfully Parsed:    {num_parsed} ({num_parsed/total_samples:.1%} of total)\")\n",
    "print(f\"Accuracy (overall):     {accuracy:.4f}\")\n",
    "print(f\"Precision (macro, parsed only): {precision:.4f}\")\n",
    "print(f\"Recall (macro, parsed only):    {recall:.4f}\")\n",
    "print(f\"F1 Score (macro, parsed only):  {f1:.4f}\")\n",
    "\n",
    "# Log to W&B if desired ---\n",
    "wandb.log({\n",
    "    f\"{split_to_evaluate}_accuracy\": accuracy,\n",
    "    f\"{split_to_evaluate}_precision_macro\": precision,\n",
    "    f\"{split_to_evaluate}_recall_macro\": recall,\n",
    "    f\"{split_to_evaluate}_f1_macro\": f1,\n",
    "    f\"{split_to_evaluate}_parsed_count\": num_parsed,\n",
    "    f\"{split_to_evaluate}_total_count\": total_samples\n",
    "})\n",
    "\n",
    "# (Optional) Clean up GPU memory\n",
    "del model\n",
    "del base_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
