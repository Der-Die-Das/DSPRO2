{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74f697d9-b897-4e86-bb14-aa34cc4d6d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.12/site-packages (4.52.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.32.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa3c1646-654e-4bde-ae71-eb8145ac7a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed257479-96e3-4aec-add0-5f756aefa28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = load_dataset(\"tau/commonsense_qa\", split=\"train[:-1000]\")\n",
    "valid_raw = load_dataset(\"tau/commonsense_qa\", split=\"train[-1000:]\")\n",
    "test_raw  = load_dataset(\"tau/commonsense_qa\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edc5f39d-0701-490a-a5eb-564a2c6e478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, max_length=80):\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.label_map = {'A':0, 'B':1, 'C':2, 'D':3, 'E':4}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        question = item[\"question\"]\n",
    "        choices = item[\"choices\"][\"text\"]\n",
    "        input_texts = [f\"{question} {choice}\" for choice in choices]\n",
    "        encoded = self.tokenizer(\n",
    "            input_texts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"],           # [5, seq_len]\n",
    "            \"attention_mask\": encoded[\"attention_mask\"], # [5, seq_len]\n",
    "            \"label\": torch.tensor(self.label_map[item[\"answerKey\"]])\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e603ae5-3958-4d0a-8027-7e8dfc15bc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTransformerHybrid(nn.Module):\n",
    "    def __init__(self, model_name=\"albert-base-v2\", hidden_size=128, num_labels=5):\n",
    "        super().__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=self.transformer.config.hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1)  # Output a score for each choice\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        batch_size, num_choices, seq_len = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, seq_len)           # [batch*num_choices, seq_len]\n",
    "        attention_mask = attention_mask.view(-1, seq_len) # [batch*num_choices, seq_len]\n",
    "\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state       # [batch*num_choices, seq_len, hidden]\n",
    "        lstm_out, _ = self.bilstm(sequence_output)        # [batch*num_choices, seq_len, hidden*2]\n",
    "        pooled = lstm_out.mean(dim=1)                     # [batch*num_choices, hidden*2]\n",
    "        logits = self.classifier(pooled)                  # [batch*num_choices, 1]\n",
    "        logits = logits.view(batch_size, num_choices)     # [batch, num_choices]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9663af2f-f530-44fa-aa17-8ad62bc66586",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"albert-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_dataset = HybridDataset(train_raw, tokenizer)\n",
    "valid_dataset = HybridDataset(valid_raw, tokenizer)\n",
    "test_dataset  = HybridDataset(test_raw, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "model = BiLSTMTransformerHybrid(model_name=model_name).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016bda95-15d6-4b02-bdb4-be139b089136",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=wandb.config.lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += input_ids.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += input_ids.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef75723a-9b15-4a90-b831-e2f1cd5734b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=1.6097, Acc=0.2042 | Val Loss=1.6093, Acc=0.2350\n",
      "Epoch 2: Train Loss=1.6096, Acc=0.2103 | Val Loss=1.6050, Acc=0.2270\n",
      "Epoch 3: Train Loss=1.5807, Acc=0.2592 | Val Loss=1.5094, Acc=0.3170\n",
      "Epoch 4: Train Loss=1.4009, Acc=0.4064 | Val Loss=1.3983, Acc=0.4180\n",
      "Epoch 5: Train Loss=1.0675, Acc=0.5774 | Val Loss=1.4494, Acc=0.4270\n",
      "Epoch 6: Train Loss=0.6351, Acc=0.7614 | Val Loss=1.8551, Acc=0.4040\n",
      "Epoch 7: Train Loss=0.2524, Acc=0.9112 | Val Loss=2.8140, Acc=0.4030\n",
      "Epoch 8: Train Loss=0.1524, Acc=0.9491 | Val Loss=2.9544, Acc=0.4030\n",
      "Epoch 9: Train Loss=0.0616, Acc=0.9809 | Val Loss=3.8757, Acc=0.3970\n",
      "Epoch 10: Train Loss=0.0368, Acc=0.9894 | Val Loss=4.1081, Acc=0.4130\n",
      "Epoch 11: Train Loss=0.0559, Acc=0.9807 | Val Loss=3.2454, Acc=0.4140\n",
      "Epoch 12: Train Loss=0.0806, Acc=0.9754 | Val Loss=3.9594, Acc=0.4010\n",
      "Epoch 13: Train Loss=0.0585, Acc=0.9825 | Val Loss=4.6006, Acc=0.3930\n",
      "Epoch 14: Train Loss=0.0269, Acc=0.9919 | Val Loss=4.6027, Acc=0.3920\n",
      "Epoch 15: Train Loss=0.0372, Acc=0.9876 | Val Loss=5.0053, Acc=0.3870\n",
      "Epoch 16: Train Loss=0.0357, Acc=0.9878 | Val Loss=4.6895, Acc=0.3970\n",
      "Epoch 17: Train Loss=0.0606, Acc=0.9824 | Val Loss=4.8000, Acc=0.3910\n",
      "Epoch 18: Train Loss=0.0258, Acc=0.9911 | Val Loss=4.8660, Acc=0.3920\n",
      "Epoch 19: Train Loss=0.0325, Acc=0.9889 | Val Loss=4.6056, Acc=0.3960\n",
      "Epoch 20: Train Loss=0.0280, Acc=0.9902 | Val Loss=5.0546, Acc=0.4020\n",
      "Epoch 21: Train Loss=0.0403, Acc=0.9881 | Val Loss=3.5265, Acc=0.3780\n",
      "Epoch 22: Train Loss=0.0814, Acc=0.9743 | Val Loss=4.6327, Acc=0.3970\n",
      "Epoch 23: Train Loss=0.0169, Acc=0.9935 | Val Loss=4.3732, Acc=0.3840\n",
      "Epoch 24: Train Loss=0.0841, Acc=0.9721 | Val Loss=4.0992, Acc=0.3780\n",
      "Epoch 25: Train Loss=0.0657, Acc=0.9772 | Val Loss=4.9543, Acc=0.3960\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▁▁▃▄▆▇██████████████████</td></tr><tr><td>train_loss</td><td>███▇▆▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▄██▇▇▇▇██▇▇▇▇▇▇▇▇▇▆▇▆▆▇</td></tr><tr><td>val_loss</td><td>▁▁▁▁▁▂▄▄▆▆▅▆▇▇█▇██▇█▅▇▇▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>25</td></tr><tr><td>train_accuracy</td><td>0.97723</td></tr><tr><td>train_loss</td><td>0.0657</td></tr><tr><td>val_accuracy</td><td>0.396</td></tr><tr><td>val_loss</td><td>4.95431</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">albert-finetune</strong> at: <a href='https://wandb.ai/aditi-sharma-00073-hochschule-luzern/commonsenseqa_albert_best-25/runs/c9c6mab9' target=\"_blank\">https://wandb.ai/aditi-sharma-00073-hochschule-luzern/commonsenseqa_albert_best-25/runs/c9c6mab9</a><br> View project at: <a href='https://wandb.ai/aditi-sharma-00073-hochschule-luzern/commonsenseqa_albert_best-25' target=\"_blank\">https://wandb.ai/aditi-sharma-00073-hochschule-luzern/commonsenseqa_albert_best-25</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250516_131307-c9c6mab9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(wandb.config.epochs):\n",
    "    train_loss, train_acc = train(model, train_loader)\n",
    "    val_loss, val_acc = evaluate(model, valid_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Acc={train_acc:.4f} | Val Loss={val_loss:.4f}, Acc={val_acc:.4f}\")\n",
    "\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch+1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_accuracy\": train_acc,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_accuracy\": val_acc\n",
    "    })\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
